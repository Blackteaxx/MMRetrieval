{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-12T10:43:20.980177Z",
     "iopub.status.busy": "2024-12-12T10:43:20.979918Z",
     "iopub.status.idle": "2024-12-12T10:43:38.821493Z",
     "shell.execute_reply": "2024-12-12T10:43:38.820813Z",
     "shell.execute_reply.started": "2024-12-12T10:43:20.980147Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.47.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n",
    "\n",
    "comp_prefix = \"data\"\n",
    "train_path = comp_prefix + \"/train.csv\"\n",
    "test_path = comp_prefix + \"/test.csv\"\n",
    "\n",
    "CV = True if len(pd.read_csv(test_path)) < 10 else False\n",
    "\n",
    "img_dir = \"data/train_images\" if CV else \"data/test_images\"\n",
    "split = \"valid\"\n",
    "\n",
    "use_CLIP = False\n",
    "use_BGE = True\n",
    "use_VisBGE = False\n",
    "\n",
    "model2path = {\n",
    "    \"clip\": \"modeloutput/1226/infonce/image/checkpoint-1750\",\n",
    "    \"bge\": \"/data/MMRetrieval/FlagEmbedding/examples/finetune/embedder/encoder_only/test_encoder_only_base_bge-large-en-v1.5/checkpoint-426\",\n",
    "    \"reranker\": \"/data/model/BAAI/bge-reranker-large\",\n",
    "    \"visbge\": \"model/BAAI/bge-visualized/Visualized_m3.pth\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils function and Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-12T10:43:38.823625Z",
     "iopub.status.busy": "2024-12-12T10:43:38.823132Z",
     "iopub.status.idle": "2024-12-12T10:43:38.836848Z",
     "shell.execute_reply": "2024-12-12T10:43:38.836041Z",
     "shell.execute_reply.started": "2024-12-12T10:43:38.823597Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def visualize(df, index, col):\n",
    "    \"\"\"可视化图片和预测结果，预测结果为posting_id的list\"\"\"\n",
    "    row = df.iloc[index]\n",
    "    preds = row[col]\n",
    "    img_dir: str = \"data/train_images\"\n",
    "    images = [df[df.posting_id == pred].image.values[0] for pred in preds]\n",
    "\n",
    "    target_title = row.title\n",
    "    target_img = row.image\n",
    "\n",
    "    titles = [df[df.posting_id == pred].title.values[0] for pred in preds]\n",
    "    images = [Image.open(os.path.join(img_dir, img)) for img in images]\n",
    "\n",
    "    rows = 5\n",
    "    cols = 5\n",
    "    fig, ax = plt.subplots(rows, cols, figsize=(20, 20))\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    for i in range(cols):\n",
    "        ax[0, i].axis(\"off\")\n",
    "\n",
    "    for i in range(rows):\n",
    "        ax[i, 0].axis(\"off\")\n",
    "\n",
    "    ax[0, 0].imshow(Image.open(os.path.join(img_dir, target_img)))\n",
    "    ax[0, 0].set_title(\n",
    "        \"\\n\".join([target_title[i : i + 10] for i in range(0, len(target_title), 10)]),\n",
    "        fontsize=12,\n",
    "    )\n",
    "\n",
    "    for i in range(1, rows):\n",
    "        for j in range(1, cols):\n",
    "            idx = (i - 1) * cols + (j - 1)\n",
    "            ax[i, j].axis(\"off\")\n",
    "            if idx < len(images):\n",
    "                ax[i, j].imshow(images[idx])\n",
    "                ax[i, j].set_title(\n",
    "                    \"\\n\".join(\n",
    "                        [\n",
    "                            titles[idx][k : k + 10]\n",
    "                            for k in range(0, len(titles[idx]), 10)\n",
    "                        ]\n",
    "                    ),\n",
    "                    fontsize=12,\n",
    "                )\n",
    "\n",
    "\n",
    "def compute_f1(col):\n",
    "    def f1(row):\n",
    "        n = len(np.intersect1d(row[\"label\"], row[col]))\n",
    "        return 2 * n / (len(row[\"label\"]) + len(row[col]))\n",
    "\n",
    "    return f1\n",
    "\n",
    "\n",
    "def compute_recall(col):\n",
    "    def recall(row):\n",
    "        n = len(np.intersect1d(row[\"label\"], row[col]))\n",
    "        return n / len(row[\"label\"])\n",
    "\n",
    "    return recall\n",
    "\n",
    "\n",
    "def compute_precision(col):\n",
    "    def precision(row):\n",
    "        n = len(np.intersect1d(row[\"label\"], row[col]))\n",
    "        if len(row[col]) == 0:\n",
    "            return 0\n",
    "        return n / len(row[col])\n",
    "\n",
    "    return precision\n",
    "\n",
    "\n",
    "def compute_AP(col, N):\n",
    "    \"\"\"compute average precision\"\"\"\n",
    "\n",
    "    def AP(row):\n",
    "        K = N\n",
    "\n",
    "        if len(row[col]) == 0:\n",
    "            return 0.0\n",
    "\n",
    "        if len(row[col]) > N:\n",
    "            pred = row[col][:N]\n",
    "        else:\n",
    "            K = len(row[col])\n",
    "            pred = row[col]\n",
    "\n",
    "        actual = row[\"label\"]\n",
    "        score = 0.0\n",
    "        num_hits = 0.0\n",
    "\n",
    "        for i, p in enumerate(pred):\n",
    "            if p in actual and p not in pred[:i]:\n",
    "                num_hits += 1.0\n",
    "                score += num_hits / (i + 1.0)\n",
    "\n",
    "        return score / min(len(actual), K)\n",
    "\n",
    "    return AP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T10:43:38.838322Z",
     "iopub.status.busy": "2024-12-12T10:43:38.837997Z",
     "iopub.status.idle": "2024-12-12T10:43:38.863566Z",
     "shell.execute_reply": "2024-12-12T10:43:38.862792Z",
     "shell.execute_reply.started": "2024-12-12T10:43:38.838287Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ShopeeDataset(Dataset):\n",
    "    def __init__(self, df, img_dir, split: str = \"train\"):\n",
    "        self.df = df\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(512),\n",
    "                transforms.Lambda(lambda img: img.convert(\"RGB\")),\n",
    "                transforms.ToTensor(),\n",
    "            ]\n",
    "        )\n",
    "        self.split = split\n",
    "        self.len = len(self.df)\n",
    "        # self.imgs = self._read_all_images()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        if self.split == \"train\":\n",
    "            query_text = row[\"query\"]\n",
    "            pos_text = row[\"pos_txt\"][0]\n",
    "            neg_text = row[\"neg_txt\"][0]\n",
    "\n",
    "            query_img_path = os.path.join(self.img_dir, row[\"image\"])\n",
    "            pos_img_path = os.path.join(self.img_dir, row[\"pos_img\"][0])\n",
    "            neg_img_path = os.path.join(self.img_dir, row[\"neg_img\"][0])\n",
    "\n",
    "            query_img = self._get_image(query_img_path)\n",
    "            pos_img = self._get_image(pos_img_path)\n",
    "            neg_img = self._get_image(neg_img_path)\n",
    "\n",
    "            return {\n",
    "                \"query\": {\n",
    "                    \"text\": query_text,\n",
    "                    \"image\": query_img,\n",
    "                },\n",
    "                \"pos\": {\n",
    "                    \"text\": pos_text,\n",
    "                    \"image\": pos_img,\n",
    "                },\n",
    "                \"neg\": {\n",
    "                    \"text\": neg_text,\n",
    "                    \"image\": neg_img,\n",
    "                },\n",
    "            }\n",
    "\n",
    "        elif self.split == \"valid\":\n",
    "            title = row[\"title\"]\n",
    "            image_path = os.path.join(self.img_dir, row[\"image\"])\n",
    "\n",
    "            img = self._get_image(image_path)\n",
    "            pil_img = Image.open(image_path)\n",
    "\n",
    "            return {\n",
    "                \"title\": title,\n",
    "                \"image\": img,\n",
    "                \"pil_image\": pil_img,\n",
    "                \"image_path\": image_path,\n",
    "            }\n",
    "\n",
    "    def _get_image(self, path):\n",
    "        img = Image.open(path)\n",
    "        img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "    def get_pil_image(self, path):\n",
    "        return Image.open(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T10:43:38.865637Z",
     "iopub.status.busy": "2024-12-12T10:43:38.865357Z",
     "iopub.status.idle": "2024-12-12T10:43:38.881523Z",
     "shell.execute_reply": "2024-12-12T10:43:38.880762Z",
     "shell.execute_reply.started": "2024-12-12T10:43:38.865606Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.amp\n",
    "\n",
    "\n",
    "def retrieval(embs, df, chunk_size=4096, threshold=None, topK=None):\n",
    "    assert (\n",
    "        threshold is not None or topK is not None\n",
    "    ), \"Either threshold or topK should be provided\"\n",
    "    assert (\n",
    "        threshold is None or topK is None\n",
    "    ), \"Only one of threshold or topK should be provided\"\n",
    "\n",
    "    embs_pt = torch.tensor(embs).cuda()\n",
    "\n",
    "    num_chunks = (embs_pt.shape[0] + chunk_size - 1) // chunk_size\n",
    "    posting_id = df.posting_id.to_list()\n",
    "    topk_posting_id = []\n",
    "\n",
    "    print(f\"Chunk Size: {chunk_size}, {num_chunks} chunks\")\n",
    "\n",
    "    for i in tqdm(range(num_chunks)):\n",
    "        start = i * chunk_size\n",
    "        end = min((i + 1) * chunk_size, embs_pt.shape[0])\n",
    "        sim = embs_pt[start:end] @ embs_pt.T\n",
    "\n",
    "        if topK is not None:\n",
    "            indices = torch.topk(sim, topK, dim=1).indices.cpu().numpy()\n",
    "            topk_posting_id.extend([[posting_id[j] for j in row] for row in indices])\n",
    "        elif threshold is not None:\n",
    "            mask = sim > threshold\n",
    "            indices = [\n",
    "                torch.nonzero(mask[j]).squeeze().cpu().numpy()\n",
    "                for j in range(mask.shape[0])\n",
    "            ]\n",
    "            indices = [np.unique(i) for i in indices]\n",
    "            sorted_indices = [\n",
    "                indices[j][np.argsort(-sim[j, indices[j]].cpu().numpy())]\n",
    "                for j in range(len(indices))\n",
    "            ]\n",
    "            topk_posting_id.extend(\n",
    "                [[posting_id[j] for j in row] for row in sorted_indices]\n",
    "            )\n",
    "\n",
    "    # clean up\n",
    "    del embs_pt\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return topk_posting_id\n",
    "\n",
    "\n",
    "def rerank(queries, preds, df, model, tokenizer, threshold=-0.1):\n",
    "    sample_size = len(queries)\n",
    "\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "\n",
    "    for i in tqdm(range(sample_size)):\n",
    "        pairs = []\n",
    "        query = queries[i]\n",
    "        pred_posting_ids = preds[i]\n",
    "\n",
    "        for p in pred_posting_ids:\n",
    "            pred = df[df.posting_id == p].title.values[0]\n",
    "            pairs.append((query, pred))\n",
    "\n",
    "        # print(f\"Query: {query}\")\n",
    "        # print(f\"Preds: {pred_posting_ids}\")\n",
    "        # print(f\"Pairs: {pairs}\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = tokenizer(\n",
    "                pairs,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\",\n",
    "                max_length=512,\n",
    "            )\n",
    "            inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "            with torch.amp.autocast(\"cuda\", torch.float16):\n",
    "                scores = (\n",
    "                    model(**inputs, return_dict=True)\n",
    "                    .logits.view(\n",
    "                        -1,\n",
    "                    )\n",
    "                    .float()\n",
    "                )\n",
    "            # select the >= threshold\n",
    "            mask = scores >= threshold\n",
    "            pred_posting_ids = [\n",
    "                pred_posting_ids[j] for j in range(len(mask)) if mask[j]\n",
    "            ]\n",
    "            preds[i] = pred_posting_ids\n",
    "\n",
    "    return preds\n",
    "\n",
    "\n",
    "def neighbor_blend(embs, threshold, chunk_size=4096):\n",
    "    embs_pt = torch.tensor(embs).cuda()\n",
    "    num_chunks = (embs_pt.shape[0] + chunk_size - 1) // chunk_size\n",
    "    res_embs = []\n",
    "\n",
    "    print(f\"Chunk Size: {chunk_size}, {num_chunks} chunks\")\n",
    "\n",
    "    for i in tqdm(range(num_chunks)):\n",
    "        start = i * chunk_size\n",
    "        end = min((i + 1) * chunk_size, embs_pt.shape[0])\n",
    "        sim = embs_pt[start:end] @ embs_pt.T\n",
    "        mask = sim > threshold\n",
    "\n",
    "        indices = [\n",
    "            torch.nonzero(mask[j]).squeeze().cpu().numpy() for j in range(mask.shape[0])\n",
    "        ]\n",
    "        indices = [np.unique(i) for i in indices]\n",
    "\n",
    "        # blend embs according to the similarity\n",
    "        for j in range(len(indices)):\n",
    "            weights = sim[j, indices[j]]\n",
    "            res_embs.append(\n",
    "                (weights.unsqueeze(1) * embs_pt[indices[j]]).sum(dim=0).cpu().numpy()\n",
    "            )\n",
    "\n",
    "    # clean up\n",
    "    del embs_pt\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    res_embs = np.stack(res_embs)\n",
    "    res_embs = res_embs / np.linalg.norm(res_embs, axis=1, keepdims=True)\n",
    "\n",
    "    return res_embs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load csv to df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T10:43:39.068700Z",
     "iopub.status.busy": "2024-12-12T10:43:39.068458Z",
     "iopub.status.idle": "2024-12-12T10:43:39.085903Z",
     "shell.execute_reply": "2024-12-12T10:43:39.085066Z",
     "shell.execute_reply.started": "2024-12-12T10:43:39.068676Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posting_id</th>\n",
       "      <th>image</th>\n",
       "      <th>image_phash</th>\n",
       "      <th>title</th>\n",
       "      <th>label_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_129225211</td>\n",
       "      <td>0000a68812bc7e98c42888dfb1c07da0.jpg</td>\n",
       "      <td>94974f937d4c2433</td>\n",
       "      <td>Paper Bag Victoria Secret</td>\n",
       "      <td>249114794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_2406599165</td>\n",
       "      <td>00117e4fc239b1b641ff08340b429633.jpg</td>\n",
       "      <td>8514fc58eafea283</td>\n",
       "      <td>Daster Batik Lengan pendek - Motif Acak / Camp...</td>\n",
       "      <td>4093212188</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         posting_id                                 image       image_phash  \\\n",
       "0   train_129225211  0000a68812bc7e98c42888dfb1c07da0.jpg  94974f937d4c2433   \n",
       "1  train_2406599165  00117e4fc239b1b641ff08340b429633.jpg  8514fc58eafea283   \n",
       "\n",
       "                                               title  label_group  \n",
       "0                          Paper Bag Victoria Secret    249114794  \n",
       "1  Daster Batik Lengan pendek - Motif Acak / Camp...   4093212188  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(train_path)\n",
    "valid_df = pd.read_json(\n",
    "    \"/data/MMRetrieval/data/valid.json\", lines=True, orient=\"records\"\n",
    ")\n",
    "\n",
    "valid_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T10:43:39.087204Z",
     "iopub.status.busy": "2024-12-12T10:43:39.086881Z",
     "iopub.status.idle": "2024-12-12T10:43:40.531513Z",
     "shell.execute_reply": "2024-12-12T10:43:40.530646Z",
     "shell.execute_reply.started": "2024-12-12T10:43:39.087178Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posting_id</th>\n",
       "      <th>image</th>\n",
       "      <th>image_phash</th>\n",
       "      <th>title</th>\n",
       "      <th>label_group</th>\n",
       "      <th>label</th>\n",
       "      <th>phash</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_129225211</td>\n",
       "      <td>0000a68812bc7e98c42888dfb1c07da0.jpg</td>\n",
       "      <td>94974f937d4c2433</td>\n",
       "      <td>Paper Bag Victoria Secret</td>\n",
       "      <td>249114794</td>\n",
       "      <td>[train_129225211, train_2278313361]</td>\n",
       "      <td>[train_129225211]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_3386243561</td>\n",
       "      <td>00039780dfc94d01db8676fe789ecd05.jpg</td>\n",
       "      <td>af3f9460c2838f0f</td>\n",
       "      <td>Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...</td>\n",
       "      <td>2937985045</td>\n",
       "      <td>[train_3386243561, train_3423213080]</td>\n",
       "      <td>[train_3386243561]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         posting_id                                 image       image_phash  \\\n",
       "0   train_129225211  0000a68812bc7e98c42888dfb1c07da0.jpg  94974f937d4c2433   \n",
       "1  train_3386243561  00039780dfc94d01db8676fe789ecd05.jpg  af3f9460c2838f0f   \n",
       "\n",
       "                                               title  label_group  \\\n",
       "0                          Paper Bag Victoria Secret    249114794   \n",
       "1  Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...   2937985045   \n",
       "\n",
       "                                  label               phash  \n",
       "0   [train_129225211, train_2278313361]   [train_129225211]  \n",
       "1  [train_3386243561, train_3423213080]  [train_3386243561]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if CV:\n",
    "    tmp = df.groupby(\"label_group\").posting_id.agg(\"unique\").to_dict()\n",
    "    df[\"label\"] = df.label_group.map(tmp)\n",
    "\n",
    "tmp = df.groupby(\"image_phash\").posting_id.agg(\"unique\").to_dict()\n",
    "df[\"phash\"] = df.image_phash.map(tmp)\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T10:43:40.533230Z",
     "iopub.status.busy": "2024-12-12T10:43:40.532831Z",
     "iopub.status.idle": "2024-12-12T10:43:47.037747Z",
     "shell.execute_reply": "2024-12-12T10:43:47.036897Z",
     "shell.execute_reply.started": "2024-12-12T10:43:40.533187Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5530933399168149 0.4221722749534632 0.9940677878488098 0.9963889508422321\n"
     ]
    }
   ],
   "source": [
    "if CV:\n",
    "    df[\"f1\"] = df.apply(compute_f1(\"phash\"), axis=1)\n",
    "    df[\"recall\"] = df.apply(compute_recall(\"phash\"), axis=1)\n",
    "    df[\"precision\"] = df.apply(compute_precision(\"phash\"), axis=1)\n",
    "    df[\"AP\"] = df.apply(compute_AP(\"phash\", 50), axis=1)\n",
    "\n",
    "    print(df.f1.mean(), df.recall.mean(), df.precision.mean(), df.AP.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLIP Model linear probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T10:43:47.041240Z",
     "iopub.status.busy": "2024-12-12T10:43:47.040969Z",
     "iopub.status.idle": "2024-12-12T10:43:47.052469Z",
     "shell.execute_reply": "2024-12-12T10:43:47.051832Z",
     "shell.execute_reply.started": "2024-12-12T10:43:47.041212Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CLIPForEmbedding(CLIPModel):\n",
    "    def forward(\n",
    "        self,\n",
    "        inputs,\n",
    "    ):\n",
    "        text_inputs = inputs[\"text\"]\n",
    "        image_inputs = inputs[\"image\"]\n",
    "\n",
    "        text_embs = self.get_text_features(**text_inputs)\n",
    "        image_embs = self.get_image_features(**image_inputs)\n",
    "\n",
    "        return {\"text\": text_embs, \"image\": image_embs}\n",
    "\n",
    "\n",
    "class CLIPForFusion(nn.Module):\n",
    "    def __init__(self, clip_model=None, processor=None):\n",
    "        super().__init__()\n",
    "        self.clip_model = clip_model\n",
    "        self.processor = processor\n",
    "\n",
    "        self.txt_hidden_dim = clip_model.projection_dim\n",
    "        self.img_hidden_dim = clip_model.projection_dim\n",
    "        self.fusion_dim = 512\n",
    "\n",
    "        self.text_fc = nn.Linear(self.txt_hidden_dim, self.fusion_dim)\n",
    "        self.image_fc = nn.Linear(self.img_hidden_dim, self.fusion_dim)\n",
    "        self.act_fn = nn.ReLU()\n",
    "\n",
    "        self.fusion_fc = nn.Linear(self.fusion_dim * 2, self.fusion_dim)\n",
    "\n",
    "        self.projector = nn.Linear(self.fusion_dim, self.fusion_dim)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_name_or_path):\n",
    "        clip_model = CLIPForEmbedding.from_pretrained(model_name_or_path)\n",
    "        processor = CLIPProcessor.from_pretrained(model_name_or_path)\n",
    "        model = cls(clip_model, processor)\n",
    "        model_path = os.path.join(model_name_or_path, \"model.pt\")\n",
    "\n",
    "        if os.path.exists(model_path):\n",
    "            logging.info(f\"Load model.pt from {model_path}\")\n",
    "            state_dict = torch.load(model_path, weights_only=True)\n",
    "            # only load the fc layers\n",
    "            missing_keys, unexpected_keys = model.load_state_dict(\n",
    "                state_dict, strict=False\n",
    "            )\n",
    "            logging.info(f\"missing keys: {missing_keys}\")\n",
    "            logging.info(f\"unexpected keys: {unexpected_keys}\")\n",
    "        else:\n",
    "            logging.warning(\n",
    "                f\"model.pt not found in {model_name_or_path}, initialize from scratch\"\n",
    "            )\n",
    "        return model\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embed = self.clip_model(inputs)\n",
    "\n",
    "        text_embs = embed[\"text\"]\n",
    "        image_embs = embed[\"image\"]\n",
    "\n",
    "        text_fusion = self.act_fn(self.text_fc(text_embs))\n",
    "        image_fusion = self.act_fn(self.image_fc(image_embs))\n",
    "\n",
    "        fusion = self.act_fn(\n",
    "            self.fusion_fc(torch.cat([text_fusion, image_fusion], dim=-1))\n",
    "        )\n",
    "\n",
    "        proj = self.act_fn(self.projector(fusion))\n",
    "\n",
    "        return proj\n",
    "\n",
    "    def encode(self, inputs):\n",
    "        embed = self.clip_model(inputs)\n",
    "        text_embs = embed[\"text\"]\n",
    "        image_embs = embed[\"image\"]\n",
    "\n",
    "        text_fusion = self.act_fn(self.text_fc(text_embs))\n",
    "        image_fusion = self.act_fn(self.image_fc(image_embs))\n",
    "\n",
    "        fusion = self.act_fn(\n",
    "            self.fusion_fc(torch.cat([text_fusion, image_fusion], dim=-1))\n",
    "        )\n",
    "\n",
    "        return fusion\n",
    "\n",
    "\n",
    "def get_collate_fn(processor):\n",
    "    def collate_fn(batch):\n",
    "        texts = [item[\"title\"] for item in batch]\n",
    "        images = [item[\"image\"] for item in batch]\n",
    "        text_encoded = processor(\n",
    "            text=texts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=77,\n",
    "        )\n",
    "        img_encoded = processor(images=images, return_tensors=\"pt\", do_rescale=False)\n",
    "\n",
    "        return {\"text\": text_encoded, \"image\": img_encoded}\n",
    "\n",
    "    return collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T10:43:47.053548Z",
     "iopub.status.busy": "2024-12-12T10:43:47.053320Z",
     "iopub.status.idle": "2024-12-12T10:43:49.337791Z",
     "shell.execute_reply": "2024-12-12T10:43:49.337074Z",
     "shell.execute_reply.started": "2024-12-12T10:43:47.053525Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_path = model2path[\"clip\"]\n",
    "\n",
    "# model = CLIPForEmbedding.from_pretrained(model_path)\n",
    "model = CLIPForFusion.from_pretrained(model_path)\n",
    "processor = CLIPProcessor.from_pretrained(model_path)\n",
    "\n",
    "\n",
    "dataset = ShopeeDataset(df, img_dir, split)\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=256,\n",
    "    num_workers=16,\n",
    "    collate_fn=get_collate_fn(processor),\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T10:43:49.339031Z",
     "iopub.status.busy": "2024-12-12T10:43:49.338755Z",
     "iopub.status.idle": "2024-12-12T10:43:49.346865Z",
     "shell.execute_reply": "2024-12-12T10:43:49.345940Z",
     "shell.execute_reply.started": "2024-12-12T10:43:49.339005Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if use_CLIP:\n",
    "    embs = []\n",
    "    # get image + text embedding into numpy array\n",
    "\n",
    "    model.eval()\n",
    "    model.cuda()\n",
    "    for batch in tqdm(loader):\n",
    "        with torch.no_grad():\n",
    "            with torch.amp.autocast(\"cuda\", dtype=torch.float16):\n",
    "                for k, v in batch.items():\n",
    "                    batch[k] = {key: val.cuda() for key, val in v.items()}\n",
    "                emb = model(batch)\n",
    "                # text_emb, image_emb = emb[\"text\"], emb[\"image\"]\n",
    "                # emb = torch.cat([text_emb, image_emb], dim=-1)\n",
    "            embs.append(emb.cpu().detach().numpy())\n",
    "\n",
    "    print(len(embs))\n",
    "    embs = np.concatenate(embs, axis=0)\n",
    "    # normalize\n",
    "    embs /= np.linalg.norm(embs, axis=1, keepdims=True)\n",
    "\n",
    "del model, processor\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_CLIP:\n",
    "    # for i in range(3):\n",
    "    #     embs = neighbor_blend(embs, 0.9)\n",
    "    # candidate = np.arange(0.85, 0.95, 0.005)\n",
    "    # for threshold in candidate:\n",
    "    threshold = 0.8\n",
    "    df[\"clip\"] = retrieval(embs, df, threshold=threshold)\n",
    "\n",
    "    if CV:\n",
    "        df[\"f1_clip\"] = df.apply(compute_f1(\"clip\"), axis=1)\n",
    "        df[\"recall_clip\"] = df.apply(compute_recall(\"clip\"), axis=1)\n",
    "        df[\"precision_clip\"] = df.apply(compute_precision(\"clip\"), axis=1)\n",
    "        df[\"AP_clip\"] = df.apply(compute_AP(\"clip\", 50), axis=1)\n",
    "\n",
    "        print(\n",
    "            threshold,\n",
    "            df.f1_clip.mean(),\n",
    "            df.recall_clip.mean(),\n",
    "            df.precision_clip.mean(),\n",
    "            df.AP_clip.mean(),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T10:43:49.348185Z",
     "iopub.status.busy": "2024-12-12T10:43:49.347935Z",
     "iopub.status.idle": "2024-12-12T10:43:50.070465Z",
     "shell.execute_reply": "2024-12-12T10:43:50.069225Z",
     "shell.execute_reply.started": "2024-12-12T10:43:49.348161Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "model_path = model2path[\"bge\"]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModel.from_pretrained(model_path)\n",
    "\n",
    "dataset = ShopeeDataset(df, img_dir, split)\n",
    "\n",
    "\n",
    "def get_collate_fn():\n",
    "    def collate_fn(batch):\n",
    "        texts = [item[\"title\"] for item in batch]\n",
    "        return texts\n",
    "\n",
    "    return collate_fn\n",
    "\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=256,\n",
    "    num_workers=16,\n",
    "    collate_fn=get_collate_fn(),\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T10:43:50.072521Z",
     "iopub.status.busy": "2024-12-12T10:43:50.071832Z",
     "iopub.status.idle": "2024-12-12T10:46:59.779784Z",
     "shell.execute_reply": "2024-12-12T10:46:59.778686Z",
     "shell.execute_reply.started": "2024-12-12T10:43:50.072480Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 134/134 [00:36<00:00,  3.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134\n"
     ]
    }
   ],
   "source": [
    "if use_BGE:\n",
    "    embs = []\n",
    "    # get text embedding into numpy array\n",
    "\n",
    "    model.eval()\n",
    "    model.cuda()\n",
    "    for batch in tqdm(loader):\n",
    "        with torch.no_grad():\n",
    "            inputs = tokenizer(\n",
    "                batch, return_tensors=\"pt\", padding=True, truncation=True\n",
    "            )\n",
    "            with torch.amp.autocast(\"cuda\", dtype=torch.float16):\n",
    "                # print(inputs)\n",
    "                inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "                output = model(**inputs)\n",
    "                emb = output.last_hidden_state[:, 0]\n",
    "            embs.append(emb.cpu().detach().numpy())\n",
    "\n",
    "    print(len(embs))\n",
    "    embs = np.concatenate(embs, axis=0)\n",
    "    # normalize\n",
    "    embs /= np.linalg.norm(embs, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "del model, tokenizer\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk Size: 4096, 9 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 30.57it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "if use_BGE:\n",
    "    df[\"bge\"] = retrieval(embs, df, topK=50)\n",
    "\n",
    "    # 挑选出valid_df中的数据\n",
    "    res = df[df.posting_id.isin(valid_df.posting_id)].copy()\n",
    "\n",
    "    # # use rerank\n",
    "    # model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    #     model2path[\"reranker\"]\n",
    "    # ).cuda()\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(model2path[\"reranker\"])\n",
    "\n",
    "    # queries = res.title.to_list()\n",
    "    # preds = res.bge.to_list()\n",
    "\n",
    "    # res[\"bge\"] = rerank(queries, preds, df, model, tokenizer, threshold=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17046612175702128 0.9469823561089816 0.10597131681877445 0.8150732498372881\n"
     ]
    }
   ],
   "source": [
    "if use_BGE and CV:\n",
    "    res[\"f1_bge\"] = res.apply(compute_f1(\"bge\"), axis=1)\n",
    "    res[\"recall_bge\"] = res.apply(compute_recall(\"bge\"), axis=1)\n",
    "    res[\"precision_bge\"] = res.apply(compute_precision(\"bge\"), axis=1)\n",
    "    res[\"AP_bge\"] = res.apply(compute_AP(\"bge\", 50), axis=1)\n",
    "\n",
    "    print(\n",
    "        res.f1_bge.mean(),\n",
    "        res.recall_bge.mean(),\n",
    "        res.precision_bge.mean(),\n",
    "        res.AP_bge.mean(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "posting_id                                        train_1641797816\n",
      "image                         00e3e2ddacb5902fbd98dcb0a5d96ab9.jpg\n",
      "image_phash                                       febc8142d6cf2e20\n",
      "title            Tali Ikat Pinggang Pria Canvas Army Military T...\n",
      "label_group                                             1037925737\n",
      "label            [train_1648915230, train_2500339995, train_397...\n",
      "phash            [train_1648915230, train_2500339995, train_397...\n",
      "f1                                                             1.0\n",
      "recall                                                         1.0\n",
      "precision                                                      1.0\n",
      "AP                                                             1.0\n",
      "bge              [train_1641797816, train_1648915230, train_341...\n",
      "f1_bge                                                    0.181818\n",
      "recall_bge                                                     1.0\n",
      "precision_bge                                                  0.1\n",
      "AP_bge                                                    0.876667\n",
      "Name: 107, dtype: object\n",
      "Predictions:  ['train_1641797816', 'train_1648915230', 'train_3414538010', 'train_2500339995', 'train_3977286900', 'train_910440218', 'train_1695564998', 'train_193792916', 'train_2834776851', 'train_2849735715', 'train_4248767492', 'train_331127324', 'train_3214034262', 'train_2742414112', 'train_2243728099', 'train_4130389297', 'train_3494183521', 'train_1567223529', 'train_3759478552', 'train_10811831', 'train_854037625', 'train_803605409', 'train_3428046068', 'train_2858902283', 'train_4137127341', 'train_2121471003', 'train_1985873476', 'train_3481673715', 'train_3741633368', 'train_2407818842', 'train_804851985', 'train_615383284', 'train_2617644869', 'train_1787495996', 'train_3344756233', 'train_3109127293', 'train_1666571189', 'train_2535681490', 'train_3293609045', 'train_1423319877', 'train_3338616318', 'train_1158330602', 'train_1834352946', 'train_4016913167', 'train_3551675088', 'train_2031509033', 'train_859469363', 'train_2834429502', 'train_4134499941', 'train_2162275562']\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m6\u001b[39m\n\u001b[1;32m      4\u001b[0m samples \u001b[38;5;241m=\u001b[39m res[res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1_bge\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.8\u001b[39m]\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mto_list()\n\u001b[0;32m----> 5\u001b[0m visualize(res, samples[index], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbge\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m visualize(res, samples[index], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[54], line 8\u001b[0m, in \u001b[0;36mvisualize\u001b[0;34m(df, index, col)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictions: \u001b[39m\u001b[38;5;124m\"\u001b[39m, preds)\n\u001b[1;32m      7\u001b[0m img_dir: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/train_images\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 8\u001b[0m images \u001b[38;5;241m=\u001b[39m [df[df\u001b[38;5;241m.\u001b[39mposting_id \u001b[38;5;241m==\u001b[39m pred]\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m pred \u001b[38;5;129;01min\u001b[39;00m preds]\n\u001b[1;32m     10\u001b[0m target_title \u001b[38;5;241m=\u001b[39m row\u001b[38;5;241m.\u001b[39mtitle\n\u001b[1;32m     11\u001b[0m target_img \u001b[38;5;241m=\u001b[39m row\u001b[38;5;241m.\u001b[39mimage\n",
      "Cell \u001b[0;32mIn[54], line 8\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictions: \u001b[39m\u001b[38;5;124m\"\u001b[39m, preds)\n\u001b[1;32m      7\u001b[0m img_dir: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/train_images\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 8\u001b[0m images \u001b[38;5;241m=\u001b[39m [df[df\u001b[38;5;241m.\u001b[39mposting_id \u001b[38;5;241m==\u001b[39m pred]\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m pred \u001b[38;5;129;01min\u001b[39;00m preds]\n\u001b[1;32m     10\u001b[0m target_title \u001b[38;5;241m=\u001b[39m row\u001b[38;5;241m.\u001b[39mtitle\n\u001b[1;32m     11\u001b[0m target_img \u001b[38;5;241m=\u001b[39m row\u001b[38;5;241m.\u001b[39mimage\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "# select the pred where f1 is below 0.6\n",
    "# if use_BGE and CV:\n",
    "#     index = 6\n",
    "#     samples = res[res[\"f1_bge\"] < 0.8].index.to_list()\n",
    "#     visualize(res, samples[index], \"bge\")\n",
    "#     visualize(res, samples[index], \"label\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualized-BGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_collate_fn(model):\n",
    "    def collate_fn(batch):\n",
    "        texts = [item[\"title\"] for item in batch]\n",
    "        imgs = [item[\"pil_image\"] for item in batch]\n",
    "\n",
    "        texts_encoded = model.tokenizer(\n",
    "            texts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "        )\n",
    "        imgs_encoded = [model.preprocess_val(img) for img in imgs]\n",
    "\n",
    "        imgs_encoded = torch.stack(imgs_encoded)\n",
    "\n",
    "        return {\n",
    "            \"text\": texts_encoded,\n",
    "            \"img\": imgs_encoded,\n",
    "        }\n",
    "\n",
    "    return collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train.modeling_visbge import Visualized_BGE\n",
    "\n",
    "model = Visualized_BGE(\n",
    "    model_name_bge=\"BAAI/bge-m3\", model_weight=model2path[\"visbge\"]\n",
    ").to(dtype=torch.float16)\n",
    "model.eval()\n",
    "\n",
    "dataset = ShopeeDataset(df, img_dir, split)\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=256,\n",
    "    num_workers=16,\n",
    "    collate_fn=get_collate_fn(model),\n",
    "    shuffle=False,\n",
    ")\n",
    "# iter(loader).__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_VisBGE:\n",
    "    embs = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader):\n",
    "            with torch.cuda.amp.autocast():\n",
    "                images = batch[\"img\"].cuda()\n",
    "                texts = batch[\"text\"]\n",
    "                for k, v in texts.items():\n",
    "                    texts[k] = v.cuda()\n",
    "                emb = model.encode_mm(images=images, texts=texts)\n",
    "            embs.append(emb.cpu().detach().numpy())\n",
    "\n",
    "    print(len(embs))\n",
    "\n",
    "    embs = np.concatenate(embs, axis=0)\n",
    "\n",
    "    # normalize\n",
    "    embs /= np.linalg.norm(embs, axis=1, keepdims=True)\n",
    "\n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_VisBGE:\n",
    "    df[\"visbge\"] = retrieval(embs, df, threshold=0.8)\n",
    "    if CV:\n",
    "        df[\"f1_visbge\"] = df.apply(compute_f1(\"visbge\"), axis=1)\n",
    "        df[\"recall_visbge\"] = df.apply(compute_recall(\"visbge\"), axis=1)\n",
    "        df[\"precision_visbge\"] = df.apply(compute_precision(\"visbge\"), axis=1)\n",
    "        df[\"AP_visbge\"] = df.apply(compute_AP(\"visbge\", 50), axis=1)\n",
    "\n",
    "        print(\n",
    "            df.f1_visbge.mean(),\n",
    "            df.recall_visbge.mean(),\n",
    "            df.precision_visbge.mean(),\n",
    "            df.AP_visbge.mean(),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the pred where f1 is below 0.6\n",
    "if CV:\n",
    "    index = 6\n",
    "    samples = df[df[\"f1_visbge\"] < 0.6].index.to_list()\n",
    "    visualize(df, samples[index], \"visbge\")\n",
    "    visualize(df, samples[index], \"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T10:46:59.781316Z",
     "iopub.status.busy": "2024-12-12T10:46:59.781041Z",
     "iopub.status.idle": "2024-12-12T10:47:00.651401Z",
     "shell.execute_reply": "2024-12-12T10:47:00.650696Z",
     "shell.execute_reply.started": "2024-12-12T10:46:59.781286Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def combine(cols):\n",
    "    def combine_(row):\n",
    "        return np.unique(np.concatenate([row[col] for col in cols]))\n",
    "\n",
    "    return combine_\n",
    "\n",
    "\n",
    "cols = [\"phash\"]\n",
    "if use_BGE:\n",
    "    cols.append(\"bge\")\n",
    "if use_CLIP:\n",
    "    cols.append(\"clip\")\n",
    "if use_VisBGE:\n",
    "    cols.append(\"visbge\")\n",
    "\n",
    "df[\"matches\"] = df.apply(combine(cols), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T10:47:00.652717Z",
     "iopub.status.busy": "2024-12-12T10:47:00.652444Z",
     "iopub.status.idle": "2024-12-12T10:47:12.630818Z",
     "shell.execute_reply": "2024-12-12T10:47:12.629950Z",
     "shell.execute_reply.started": "2024-12-12T10:47:00.652691Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if CV:\n",
    "    df[\"f1\"] = df.apply(compute_f1(\"matches\"), axis=1)\n",
    "    df[\"recall\"] = df.apply(compute_recall(\"matches\"), axis=1)\n",
    "    df[\"precision\"] = df.apply(compute_precision(\"matches\"), axis=1)\n",
    "    df[\"AP\"] = df.apply(compute_AP(\"matches\", 50), axis=1)\n",
    "\n",
    "    print(\n",
    "        df.f1.mean(),\n",
    "        df.recall.mean(),\n",
    "        df.precision.mean(),\n",
    "        df.AP.mean(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T10:47:12.632177Z",
     "iopub.status.busy": "2024-12-12T10:47:12.631906Z",
     "iopub.status.idle": "2024-12-12T10:47:12.703808Z",
     "shell.execute_reply": "2024-12-12T10:47:12.702909Z",
     "shell.execute_reply.started": "2024-12-12T10:47:12.632151Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "submission = df[[\"posting_id\", \"matches\"]]\n",
    "submission[\"matches\"] = submission[\"matches\"].apply(lambda x: \" \".join(x))\n",
    "submission.columns = [[\"posting_id\", \"matches\"]]\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T10:47:12.705058Z",
     "iopub.status.busy": "2024-12-12T10:47:12.704805Z",
     "iopub.status.idle": "2024-12-12T10:47:12.833496Z",
     "shell.execute_reply": "2024-12-12T10:47:12.832665Z",
     "shell.execute_reply.started": "2024-12-12T10:47:12.705032Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T10:47:12.834807Z",
     "iopub.status.busy": "2024-12-12T10:47:12.834512Z",
     "iopub.status.idle": "2024-12-12T10:47:12.898904Z",
     "shell.execute_reply": "2024-12-12T10:47:12.898072Z",
     "shell.execute_reply.started": "2024-12-12T10:47:12.834755Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv(\"/kaggle/working/submission.csv\")\n",
    "sub"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 1878097,
     "sourceId": 24286,
     "sourceType": "competition"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 1030,
     "modelInstanceId": 3348,
     "sourceId": 4556,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 189204,
     "modelInstanceId": 166883,
     "sourceId": 195722,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 189358,
     "modelInstanceId": 167039,
     "sourceId": 195906,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
