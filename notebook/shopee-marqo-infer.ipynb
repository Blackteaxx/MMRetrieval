{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Marqo Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "# 获取当前文件的目录\n",
    "current_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "\n",
    "# 获取上一级目录\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "# 将上一级目录添加到 sys.path\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from transformers import AutoModel, AutoProcessor\n",
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from train.marqo_fashionSigLIP import MarqoFashionSigLIP, MarqoFashionSigLIPProcessor\n",
    "\n",
    "model_name = \"/data/model/Marqo/marqo-ecommerce-embeddings-L\"\n",
    "# model_name = 'Marqo/marqo-ecommerce-embeddings-B'\n",
    "\n",
    "model = MarqoFashionSigLIP.from_pretrained(model_name)\n",
    "processor = MarqoFashionSigLIPProcessor.from_pretrained(model_name)\n",
    "\n",
    "print(\"Finish Loading Model\")\n",
    "\n",
    "model\n",
    "\n",
    "# img = Image.open(\n",
    "#     requests.get(\n",
    "#         \"https://raw.githubusercontent.com/marqo-ai/marqo-ecommerce-embeddings/refs/heads/main/images/dining-chairs.png\",\n",
    "#         stream=True,\n",
    "#     ).raw\n",
    "# ).convert(\"RGB\")\n",
    "# image = [img]\n",
    "# text = [\"dining chairs\", \"a laptop\", \"toothbrushes\"]\n",
    "# processed = processor(\n",
    "#     text=text, images=image, padding=\"max_length\", return_tensors=\"pt\"\n",
    "# )\n",
    "# processor.image_processor.do_rescale = False\n",
    "# with torch.no_grad():\n",
    "#     image_features = model.get_image_features(processed[\"pixel_values\"], normalize=True)\n",
    "#     text_features = model.get_text_features(processed[\"input_ids\"], normalize=True)\n",
    "\n",
    "#     text_probs = (100 * image_features @ text_features.T).softmax(dim=-1)\n",
    "\n",
    "# print(text_probs)\n",
    "# [1.0000e+00, 8.3131e-12, 5.2173e-12]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "class ShopeeDataset(Dataset):\n",
    "    def __init__(self, df, img_dir, split: str = \"train\"):\n",
    "        self.df = df\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(256),\n",
    "                transforms.CenterCrop(224),\n",
    "                # convert to RGB\n",
    "                transforms.Lambda(lambda img: img.convert(\"RGB\")),\n",
    "                transforms.ToTensor(),\n",
    "            ]\n",
    "        )\n",
    "        self.split = split\n",
    "        self.len = len(self.df)\n",
    "        # self.imgs = self._read_all_images()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        if self.split == \"train\":\n",
    "            query_text = row[\"query\"]\n",
    "            pos_text = row[\"pos_txt\"][0]\n",
    "            neg_text = row[\"neg_txt\"][0]\n",
    "\n",
    "            query_img_path = os.path.join(self.img_dir, row[\"image\"])\n",
    "            pos_img_path = os.path.join(self.img_dir, row[\"pos_img\"][0])\n",
    "            neg_img_path = os.path.join(self.img_dir, row[\"neg_img\"][0])\n",
    "\n",
    "            query_img = self._get_image(query_img_path)\n",
    "            pos_img = self._get_image(pos_img_path)\n",
    "            neg_img = self._get_image(neg_img_path)\n",
    "\n",
    "            return {\n",
    "                \"query\": {\n",
    "                    \"text\": query_text,\n",
    "                    \"image\": query_img,\n",
    "                },\n",
    "                \"pos\": {\n",
    "                    \"text\": pos_text,\n",
    "                    \"image\": pos_img,\n",
    "                },\n",
    "                \"neg\": {\n",
    "                    \"text\": neg_text,\n",
    "                    \"image\": neg_img,\n",
    "                },\n",
    "            }\n",
    "\n",
    "        elif self.split == \"valid\":\n",
    "            title = row[\"title\"]\n",
    "            image_path = os.path.join(self.img_dir, row[\"image\"])\n",
    "\n",
    "            img = self._get_image(image_path)\n",
    "            pil_img = Image.open(image_path)\n",
    "\n",
    "            return {\n",
    "                \"title\": title,\n",
    "                \"image\": img,\n",
    "                \"pil_image\": pil_img,\n",
    "                \"image_path\": image_path,\n",
    "            }\n",
    "\n",
    "    def _get_image(self, path):\n",
    "        img = Image.open(path)\n",
    "        img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "    def get_pil_image(self, path):\n",
    "        return Image.open(path)\n",
    "\n",
    "\n",
    "def get_collate_fn(processor):\n",
    "    def collate_fn(batch):\n",
    "        images = [item[\"image\"] for item in batch]\n",
    "        texts = [item[\"title\"] for item in batch]\n",
    "\n",
    "        processed = processor(\n",
    "            text=texts,\n",
    "            images=images,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        processor.image_processor.do_rescale = False\n",
    "\n",
    "        return processed\n",
    "\n",
    "    return collate_fn\n",
    "\n",
    "\n",
    "def visualize(df, index, col):\n",
    "    \"\"\"可视化图片和预测结果，预测结果为posting_id的list\"\"\"\n",
    "    row = df.iloc[index]\n",
    "    preds = row[col]\n",
    "    img_dir: str = \"../data/train_images\"\n",
    "    images = [df[df.posting_id == pred].image.values[0] for pred in preds]\n",
    "\n",
    "    target_title = row.title\n",
    "    target_img = row.image\n",
    "\n",
    "    titles = [df[df.posting_id == pred].title.values[0] for pred in preds]\n",
    "    images = [Image.open(os.path.join(img_dir, img)) for img in images]\n",
    "\n",
    "    rows = 5\n",
    "    cols = 5\n",
    "    fig, ax = plt.subplots(rows, cols, figsize=(20, 20))\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    for i in range(cols):\n",
    "        ax[0, i].axis(\"off\")\n",
    "\n",
    "    for i in range(rows):\n",
    "        ax[i, 0].axis(\"off\")\n",
    "\n",
    "    ax[0, 0].imshow(Image.open(os.path.join(img_dir, target_img)))\n",
    "    ax[0, 0].set_title(\n",
    "        \"\\n\".join([target_title[i : i + 10] for i in range(0, len(target_title), 10)]),\n",
    "        fontsize=12,\n",
    "    )\n",
    "\n",
    "    for i in range(1, rows):\n",
    "        for j in range(1, cols):\n",
    "            idx = (i - 1) * cols + (j - 1)\n",
    "            ax[i, j].axis(\"off\")\n",
    "            if idx < len(images):\n",
    "                ax[i, j].imshow(images[idx])\n",
    "                ax[i, j].set_title(\n",
    "                    \"\\n\".join(\n",
    "                        [\n",
    "                            titles[idx][k : k + 10]\n",
    "                            for k in range(0, len(titles[idx]), 10)\n",
    "                        ]\n",
    "                    ),\n",
    "                    fontsize=12,\n",
    "                )\n",
    "\n",
    "\n",
    "def compute_f1(col):\n",
    "    def f1(row):\n",
    "        n = len(np.intersect1d(row[\"label\"], row[col]))\n",
    "        return 2 * n / (len(row[\"label\"]) + len(row[col]))\n",
    "\n",
    "    return f1\n",
    "\n",
    "\n",
    "def compute_recall(col):\n",
    "    def recall(row):\n",
    "        n = len(np.intersect1d(row[\"label\"], row[col]))\n",
    "        return n / len(row[\"label\"])\n",
    "\n",
    "    return recall\n",
    "\n",
    "\n",
    "def compute_precision(col):\n",
    "    def precision(row):\n",
    "        n = len(np.intersect1d(row[\"label\"], row[col]))\n",
    "        return n / len(row[col])\n",
    "\n",
    "    return precision\n",
    "\n",
    "\n",
    "def compute_precision_K(col, K):\n",
    "    def precision(row):\n",
    "        n = len(np.intersect1d(row[\"label\"], row[col][:K]))\n",
    "        return n / K\n",
    "\n",
    "    return precision\n",
    "\n",
    "\n",
    "def compute_AP(col, N):\n",
    "    \"\"\"compute average precision\"\"\"\n",
    "\n",
    "    def AP(row):\n",
    "        n = len(np.intersect1d(row[\"label\"], row[col]))\n",
    "        max_n = min(len(row[col]), N)\n",
    "        if n == 0:\n",
    "            return 0\n",
    "        return (\n",
    "            sum(\n",
    "                [\n",
    "                    compute_precision_K(col, i)(row)\n",
    "                    for i in range(1, max_n + 1)\n",
    "                    if row[col][i - 1] in row[\"label\"]\n",
    "                ]\n",
    "            )\n",
    "            / max_n\n",
    "        )\n",
    "\n",
    "    return AP\n",
    "\n",
    "\n",
    "def retrieval(embs, df, chunk_size=4096, threshold=None, topK=None):\n",
    "    assert (\n",
    "        threshold is not None or topK is not None\n",
    "    ), \"Either threshold or topK should be provided\"\n",
    "    assert (\n",
    "        threshold is None or topK is None\n",
    "    ), \"Only one of threshold or topK should be provided\"\n",
    "\n",
    "    embs_pt = torch.tensor(embs).cuda()\n",
    "\n",
    "    num_chunks = (embs_pt.shape[0] + chunk_size - 1) // chunk_size\n",
    "    posting_id = df.posting_id.to_list()\n",
    "    topk_posting_id = []\n",
    "\n",
    "    print(f\"Chunk Size: {chunk_size}, {num_chunks} chunks\")\n",
    "\n",
    "    for i in tqdm(range(num_chunks)):\n",
    "        start = i * chunk_size\n",
    "        end = min((i + 1) * chunk_size, embs_pt.shape[0])\n",
    "        sim = embs_pt[start:end] @ embs_pt.T\n",
    "\n",
    "        if topK is not None:\n",
    "            indices = torch.topk(sim, topK, dim=1).indices.cpu().numpy()\n",
    "            topk_posting_id.extend([[posting_id[j] for j in row] for row in indices])\n",
    "        elif threshold is not None:\n",
    "            mask = sim > threshold\n",
    "            indices = [\n",
    "                torch.nonzero(mask[j]).squeeze().cpu().numpy()\n",
    "                for j in range(mask.shape[0])\n",
    "            ]\n",
    "            indices = [np.unique(i) for i in indices]\n",
    "            sorted_indices = [\n",
    "                indices[j][np.argsort(-sim[j, indices[j]].cpu().numpy())]\n",
    "                for j in range(len(indices))\n",
    "            ]\n",
    "            topk_posting_id.extend(\n",
    "                [[posting_id[j] for j in row] for row in sorted_indices]\n",
    "            )\n",
    "\n",
    "    # clean up\n",
    "    del embs_pt\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return topk_posting_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "df = pd.read_csv(\"/data/MMRetrieval/data/train.csv\")\n",
    "img_dir = \"/data/MMRetrieval/data/train_images\"\n",
    "split = \"valid\"\n",
    "\n",
    "dataset = ShopeeDataset(df, img_dir, split)\n",
    "collate_fn = get_collate_fn(processor)\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    "    num_workers=16,\n",
    "    collate_fn=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "embs = []\n",
    "\n",
    "model.eval()\n",
    "model.cuda()\n",
    "for batch in tqdm(dataloader):\n",
    "    with torch.no_grad():\n",
    "        for k, v in batch.items():\n",
    "            batch[k] = v.cuda() if isinstance(v, torch.Tensor) else v\n",
    "\n",
    "        with torch.amp.autocast(\"cuda\", torch.float16):\n",
    "            image_features = model.get_image_features(\n",
    "                batch[\"pixel_values\"], normalize=True\n",
    "            )\n",
    "            text_features = model.get_text_features(batch[\"input_ids\"], normalize=True)\n",
    "\n",
    "        emb = torch.cat((image_features, text_features), dim=1).detach().cpu().numpy()\n",
    "        emb /= np.linalg.norm(emb, axis=1, keepdims=True)\n",
    "\n",
    "        embs.append(emb)\n",
    "\n",
    "embs = np.concatenate(embs, axis=0)\n",
    "\n",
    "del model, processor\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.73\n",
    "\n",
    "for threshold in np.linspace(0.7, 0.8, 20):\n",
    "    df[\"marqo\"] = retrieval(embs, df, threshold=threshold, chunk_size=4096)\n",
    "\n",
    "    tmp = df.groupby(\"label_group\").posting_id.agg(\"unique\").to_dict()\n",
    "    df[\"label\"] = df.label_group.map(tmp)\n",
    "\n",
    "    df[\"f1\"] = df.apply(compute_f1(\"marqo\"), axis=1)\n",
    "    df[\"recall\"] = df.apply(compute_recall(\"marqo\"), axis=1)\n",
    "    df[\"precision\"] = df.apply(compute_precision(\"marqo\"), axis=1)\n",
    "    df[\"AP\"] = df.apply(compute_AP(\"marqo\", 50), axis=1)\n",
    "\n",
    "    print(f\"Threshold: {threshold}\")\n",
    "    print(f\"F1: {df.f1.mean()}\")\n",
    "    print(f\"Recall: {df.recall.mean()}\")\n",
    "    print(f\"Precision: {df.precision.mean()}\")\n",
    "    print(f\"AP@50: {df.AP.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 1\n",
    "samples = df[df[\"f1\"] < 0.6].index.to_list()\n",
    "visualize(df, samples[index], \"marqo\")\n",
    "visualize(df, samples[index], \"label\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
