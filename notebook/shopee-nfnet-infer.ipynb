{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hutu/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "import cv2\n",
    "import timm  # used for pretrained models\n",
    "\n",
    "import albumentations  # used for image augmentations\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "import math\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    DATA_DIR = \"../data/train_images\"\n",
    "    TRAIN_CSV = \"../data/train.csv\"\n",
    "    MODEL_PATH = \"model_14_3.95628162292398.pt\"\n",
    "\n",
    "    IMG_SIZE = 512\n",
    "    MEAN = [0.485, 0.456, 0.406]\n",
    "    STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "    EPOCHS = 15  # Try 15 epochs\n",
    "    BATCH_SIZE = 64\n",
    "\n",
    "    NUM_WORKERS = 16\n",
    "    DEVICE = \"cuda\"\n",
    "\n",
    "    CLASSES = 11014\n",
    "    SCALE = 30\n",
    "    MARGIN = 0.5\n",
    "\n",
    "    MODEL_NAME = \"eca_nfnet_l0\"\n",
    "    FC_DIM = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(df, index, col):\n",
    "    \"\"\"可视化图片和预测结果，预测结果为posting_id的list\"\"\"\n",
    "    row = df.iloc[index]\n",
    "    preds = row[col]\n",
    "    img_dir: str = \"data/train_images\"\n",
    "    images = [df[df.posting_id == pred].image.values[0] for pred in preds]\n",
    "\n",
    "    target_title = row.title\n",
    "    target_img = row.image\n",
    "\n",
    "    titles = [df[df.posting_id == pred].title.values[0] for pred in preds]\n",
    "    images = [Image.open(os.path.join(img_dir, img)) for img in images]\n",
    "\n",
    "    rows = 5\n",
    "    cols = 5\n",
    "    fig, ax = plt.subplots(rows, cols, figsize=(20, 20))\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    for i in range(cols):\n",
    "        ax[0, i].axis(\"off\")\n",
    "\n",
    "    for i in range(rows):\n",
    "        ax[i, 0].axis(\"off\")\n",
    "\n",
    "    ax[0, 0].imshow(Image.open(os.path.join(img_dir, target_img)))\n",
    "    ax[0, 0].set_title(\n",
    "        \"\\n\".join([target_title[i : i + 10] for i in range(0, len(target_title), 10)]),\n",
    "        fontsize=12,\n",
    "    )\n",
    "\n",
    "    for i in range(1, rows):\n",
    "        for j in range(1, cols):\n",
    "            idx = (i - 1) * cols + (j - 1)\n",
    "            ax[i, j].axis(\"off\")\n",
    "            if idx < len(images):\n",
    "                ax[i, j].imshow(images[idx])\n",
    "                ax[i, j].set_title(\n",
    "                    \"\\n\".join(\n",
    "                        [\n",
    "                            titles[idx][k : k + 10]\n",
    "                            for k in range(0, len(titles[idx]), 10)\n",
    "                        ]\n",
    "                    ),\n",
    "                    fontsize=12,\n",
    "                )\n",
    "\n",
    "\n",
    "def compute_f1(col):\n",
    "    def f1(row):\n",
    "        n = len(np.intersect1d(row[\"label\"], row[col]))\n",
    "        return 2 * n / (len(row[\"label\"]) + len(row[col]))\n",
    "\n",
    "    return f1\n",
    "\n",
    "\n",
    "def compute_recall(col):\n",
    "    def recall(row):\n",
    "        n = len(np.intersect1d(row[\"label\"], row[col]))\n",
    "        return n / len(row[\"label\"])\n",
    "\n",
    "    return recall\n",
    "\n",
    "\n",
    "def compute_precision(col):\n",
    "    def precision(row):\n",
    "        n = len(np.intersect1d(row[\"label\"], row[col]))\n",
    "        return n / len(row[col])\n",
    "\n",
    "    return precision\n",
    "\n",
    "\n",
    "def compute_precision_K(col, K):\n",
    "    def precision(row):\n",
    "        n = len(np.intersect1d(row[\"label\"], row[col][:K]))\n",
    "        return n / K\n",
    "\n",
    "    return precision\n",
    "\n",
    "\n",
    "def compute_AP(col, N):\n",
    "    \"\"\"compute average precision\"\"\"\n",
    "\n",
    "    def AP(row):\n",
    "        n = len(np.intersect1d(row[\"label\"], row[col]))\n",
    "        max_n = min(len(row[col]), N)\n",
    "        if n == 0:\n",
    "            return 0\n",
    "        return (\n",
    "            sum(\n",
    "                [\n",
    "                    compute_precision_K(col, i)(row)\n",
    "                    for i in range(1, max_n + 1)\n",
    "                    if row[col][i - 1] in row[\"label\"]\n",
    "                ]\n",
    "            )\n",
    "            / max_n\n",
    "        )\n",
    "\n",
    "    return AP\n",
    "\n",
    "\n",
    "def retrieval(embs, df, chunk_size=4096, threshold=None, topK=None):\n",
    "    assert (\n",
    "        threshold is not None or topK is not None\n",
    "    ), \"Either threshold or topK should be provided\"\n",
    "    assert (\n",
    "        threshold is None or topK is None\n",
    "    ), \"Only one of threshold or topK should be provided\"\n",
    "\n",
    "    embs_pt = torch.tensor(embs).cuda()\n",
    "\n",
    "    num_chunks = (embs_pt.shape[0] + chunk_size - 1) // chunk_size\n",
    "    posting_id = df.posting_id.to_list()\n",
    "    topk_posting_id = []\n",
    "\n",
    "    print(f\"Chunk Size: {chunk_size}, {num_chunks} chunks\")\n",
    "\n",
    "    for i in tqdm(range(num_chunks)):\n",
    "        start = i * chunk_size\n",
    "        end = min((i + 1) * chunk_size, embs_pt.shape[0])\n",
    "        sim = embs_pt[start:end] @ embs_pt.T\n",
    "\n",
    "        if topK is not None:\n",
    "            indices = torch.topk(sim, topK, dim=1).indices.cpu().numpy()\n",
    "            topk_posting_id.extend([[posting_id[j] for j in row] for row in indices])\n",
    "        elif threshold is not None:\n",
    "            mask = sim > threshold\n",
    "            indices = [\n",
    "                torch.nonzero(mask[j]).squeeze().cpu().numpy()\n",
    "                for j in range(mask.shape[0])\n",
    "            ]\n",
    "            indices = [np.unique(i) for i in indices]\n",
    "            sorted_indices = [\n",
    "                indices[j][np.argsort(-sim[j, indices[j]].cpu().numpy())]\n",
    "                for j in range(len(indices))\n",
    "            ]\n",
    "            topk_posting_id.extend(\n",
    "                [[posting_id[j] for j in row] for row in sorted_indices]\n",
    "            )\n",
    "\n",
    "    # clean up\n",
    "    del embs_pt\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return topk_posting_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShopeeDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.root_dir = Config.DATA_DIR\n",
    "        self.transform = transform\n",
    "        self.length = len(df)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        img_path = os.path.join(self.root_dir, row.image)\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        label = row.label_group\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented[\"image\"]\n",
    "\n",
    "        return {\"image\": image, \"label\": torch.tensor(label).long()}\n",
    "\n",
    "\n",
    "def get_test_transforms():\n",
    "    return albumentations.Compose(\n",
    "        [\n",
    "            albumentations.Resize(Config.IMG_SIZE, Config.IMG_SIZE, always_apply=True),\n",
    "            albumentations.Normalize(),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mish_func(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, i):\n",
    "        result = i * torch.tanh(F.softplus(i))\n",
    "        ctx.save_for_backward(i)\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        i = ctx.saved_tensors[0]\n",
    "\n",
    "        v = 1.0 + i.exp()\n",
    "        h = v.log()\n",
    "        grad_gh = 1.0 / h.cosh().pow_(2)\n",
    "\n",
    "        # Note that grad_hv * grad_vx = sigmoid(x)\n",
    "        # grad_hv = 1./v\n",
    "        # grad_vx = i.exp()\n",
    "\n",
    "        grad_hx = i.sigmoid()\n",
    "\n",
    "        grad_gx = grad_gh * grad_hx  # grad_hv * grad_vx\n",
    "\n",
    "        grad_f = torch.tanh(F.softplus(i)) + i * grad_gx\n",
    "\n",
    "        return grad_output * grad_f\n",
    "\n",
    "\n",
    "class Mish(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        print(\"Mish initialized\")\n",
    "        pass\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        return Mish_func.apply(input_tensor)\n",
    "\n",
    "\n",
    "def replace_activations(model, existing_layer, new_layer):\n",
    "    for name, module in reversed(model._modules.items()):\n",
    "        if len(list(module.children())) > 0:\n",
    "            model._modules[name] = replace_activations(\n",
    "                module, existing_layer, new_layer\n",
    "            )\n",
    "\n",
    "        if type(module) is existing_layer:\n",
    "            layer_new = new_layer\n",
    "            model._modules[name] = layer_new\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArcMarginProduct(nn.Module):\n",
    "    \"\"\"\n",
    "    ArcFace Layer, which can be directly integrated into the last layer of the network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features,\n",
    "        out_features,\n",
    "        scale=30.0,\n",
    "        margin=0.50,\n",
    "        easy_margin=False,\n",
    "        ls_eps=0.0,\n",
    "    ):\n",
    "        super(ArcMarginProduct, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.scale = scale\n",
    "        self.margin = margin\n",
    "        self.ls_eps = ls_eps\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "\n",
    "        self.easy_margin = easy_margin\n",
    "        self.cos_m = math.cos(margin)\n",
    "        self.sin_m = math.sin(margin)\n",
    "        self.th = math.cos(math.pi - margin)\n",
    "        self.mm = math.sin(math.pi - margin) * margin\n",
    "\n",
    "        self.ce = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def forward(self, input, label):\n",
    "        # --------------------------- cos(theta) & phi(theta) ---------------------------\n",
    "        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n",
    "        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n",
    "        phi = cosine * self.cos_m - sine * self.sin_m\n",
    "        if self.easy_margin:\n",
    "            phi = torch.where(cosine > 0, phi, cosine)\n",
    "        else:\n",
    "            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n",
    "        # --------------------------- convert label to one-hot ---------------------------\n",
    "        # one_hot = torch.zeros(cosine.size(), device='cuda')\n",
    "        one_hot = torch.zeros(cosine.size(), device=\"cuda\")\n",
    "        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n",
    "\n",
    "        # -------------torch.where(out_i = {x_i if condition_i else y_i) -------------\n",
    "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
    "        output *= self.scale\n",
    "\n",
    "        return output, self.ce(output, label)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "\n",
    "class ShopeeModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_classes=Config.CLASSES,\n",
    "        model_name=Config.MODEL_NAME,\n",
    "        fc_dim=Config.FC_DIM,\n",
    "        margin=Config.MARGIN,\n",
    "        scale=Config.SCALE,\n",
    "        use_fc=True,\n",
    "        pretrained=True,\n",
    "    ):\n",
    "        super(ShopeeModel, self).__init__()\n",
    "        print(f\"Building Model Backbone for {model_name} model\")\n",
    "\n",
    "        self.backbone = timm.create_model(model_name, pretrained=pretrained)\n",
    "\n",
    "        if model_name == \"resnext50_32x4d\":\n",
    "            final_in_features = self.backbone.fc.in_features\n",
    "            self.backbone.fc = nn.Identity()\n",
    "            self.backbone.global_pool = nn.Identity()\n",
    "\n",
    "        elif \"efficientnet\" in model_name:\n",
    "            final_in_features = self.backbone.classifier.in_features\n",
    "            self.backbone.classifier = nn.Identity()\n",
    "            self.backbone.global_pool = nn.Identity()\n",
    "\n",
    "        elif \"nfnet\" in model_name:\n",
    "            final_in_features = self.backbone.head.fc.in_features\n",
    "            self.backbone.head.fc = nn.Identity()\n",
    "            self.backbone.head.global_pool = nn.Identity()\n",
    "\n",
    "        self.pooling = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        self.use_fc = use_fc\n",
    "\n",
    "        if use_fc:\n",
    "            self.dropout = nn.Dropout(p=0.1)\n",
    "            self.fc = nn.Linear(final_in_features, fc_dim)\n",
    "            self.bn = nn.BatchNorm1d(fc_dim)\n",
    "            self.reset_parameters()\n",
    "            final_in_features = fc_dim\n",
    "\n",
    "        self.final = ArcMarginProduct(\n",
    "            final_in_features, n_classes, scale=scale, margin=margin\n",
    "        )\n",
    "\n",
    "    def forward(self, image, label):\n",
    "        features = self.extract_features(image)\n",
    "        # logits, loss = self.final(features, label)\n",
    "        return features\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_normal_(self.fc.weight)\n",
    "        nn.init.constant_(self.fc.bias, 0)\n",
    "        nn.init.constant_(self.bn.weight, 1)\n",
    "        nn.init.constant_(self.bn.bias, 0)\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = self.backbone(x)\n",
    "        x = self.pooling(x).view(batch_size, -1)\n",
    "\n",
    "        if self.use_fc:\n",
    "            x = self.dropout(x)\n",
    "            x = self.fc(x)\n",
    "            x = self.bn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Model Backbone for eca_nfnet_l0 model\n",
      "Mish initialized\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(Config.TRAIN_CSV)\n",
    "\n",
    "model = ShopeeModel(model_name=Config.MODEL_NAME)\n",
    "model.eval()\n",
    "\n",
    "model = replace_activations(model, torch.nn.SiLU, Mish())\n",
    "\n",
    "state_dict = torch.load(Config.MODEL_PATH, weights_only=True)\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/536 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 536/536 [04:46<00:00,  1.87it/s]\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import DataParallel\n",
    "\n",
    "dataset = ShopeeDataset(df, transform=get_test_transforms())\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=Config.BATCH_SIZE,\n",
    "    num_workers=Config.NUM_WORKERS,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "embs = []\n",
    "model = DataParallel(model)\n",
    "model.cuda()\n",
    "for batch in tqdm(loader):\n",
    "    img = batch[\"image\"].cuda()\n",
    "    label = batch[\"label\"].cuda()\n",
    "    with torch.no_grad():\n",
    "        feat = model(img, label)\n",
    "        image_embeddings = feat.detach().cpu().numpy()\n",
    "        embs.append(image_embeddings)\n",
    "\n",
    "image_embeddings = np.concatenate(embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = df.groupby(\"label_group\").posting_id.agg(\"unique\").to_dict()\n",
    "df[\"label\"] = df.label_group.map(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk Size: 4096, 9 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:09<00:00,  1.07s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8663392386832592, 0.906322577113404, 0.8843458494742749, 0.8761870995071006)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize embeddings\n",
    "image_embeddings /= np.linalg.norm(image_embeddings, 2, axis=1, keepdims=True)\n",
    "\n",
    "df[\"image_retrieval\"] = retrieval(image_embeddings, df, threshold=0.5)\n",
    "df[\"f1\"] = df.apply(compute_f1(\"image_retrieval\"), axis=1)\n",
    "df[\"recall\"] = df.apply(compute_recall(\"image_retrieval\"), axis=1)\n",
    "df[\"precision\"] = df.apply(compute_precision(\"image_retrieval\"), axis=1)\n",
    "df[\"AP\"] = df.apply(compute_AP(\"image_retrieval\", 50), axis=1)\n",
    "\n",
    "df[\"f1\"].mean(), df[\"recall\"].mean(), df[\"precision\"].mean(), df[\"AP\"].mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
