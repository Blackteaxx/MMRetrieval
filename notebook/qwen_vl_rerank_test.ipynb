{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You current version of `autoawq` does not support module quantization skipping, please upgrade `autoawq` package to at least 0.1.8.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mQwen2VLForConditionalGeneration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../model/Qwen/Qwen2-VL-2B-Instruct-AWQ\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m processor \u001b[38;5;241m=\u001b[39m AutoProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/data/model/Qwen/Qwen2-VL-2B-Instruct-AWQ\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Image\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/modeling_utils.py:3659\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3657\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_quantized \u001b[38;5;129;01mor\u001b[39;00m quantization_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3658\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pre_quantized:\n\u001b[0;32m-> 3659\u001b[0m         config\u001b[38;5;241m.\u001b[39mquantization_config \u001b[38;5;241m=\u001b[39m \u001b[43mAutoHfQuantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge_quantization_configs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3660\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantization_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantization_config\u001b[49m\n\u001b[1;32m   3661\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3662\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3663\u001b[0m         config\u001b[38;5;241m.\u001b[39mquantization_config \u001b[38;5;241m=\u001b[39m quantization_config\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/quantizers/auto.py:173\u001b[0m, in \u001b[0;36mAutoHfQuantizer.merge_quantization_configs\u001b[0;34m(cls, quantization_config, quantization_config_from_args)\u001b[0m\n\u001b[1;32m    170\u001b[0m     warning_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(quantization_config, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m--> 173\u001b[0m     quantization_config \u001b[38;5;241m=\u001b[39m \u001b[43mAutoQuantizationConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(quantization_config, (GPTQConfig, AwqConfig, FbgemmFp8Config))\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m quantization_config_from_args \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    178\u001b[0m ):\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;66;03m# special case for GPTQ / AWQ / FbgemmFp8 config collision\u001b[39;00m\n\u001b[1;32m    180\u001b[0m     loading_attr_dict \u001b[38;5;241m=\u001b[39m quantization_config_from_args\u001b[38;5;241m.\u001b[39mget_loading_attributes()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/quantizers/auto.py:103\u001b[0m, in \u001b[0;36mAutoQuantizationConfig.from_dict\u001b[0;34m(cls, quantization_config_dict)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     98\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown quantization type, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquant_method\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - supported types are:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(AUTO_QUANTIZER_MAPPING\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    100\u001b[0m     )\n\u001b[1;32m    102\u001b[0m target_cls \u001b[38;5;241m=\u001b[39m AUTO_QUANTIZATION_CONFIG_MAPPING[quant_method]\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtarget_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquantization_config_dict\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/utils/quantization_config.py:102\u001b[0m, in \u001b[0;36mQuantizationConfigMixin.from_dict\u001b[0;34m(cls, config_dict, return_unused_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_dict\u001b[39m(\u001b[38;5;28mcls\u001b[39m, config_dict, return_unused_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     87\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;124;03m    Instantiates a [`QuantizationConfigMixin`] from a Python dictionary of parameters.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m        [`QuantizationConfigMixin`]: The configuration object instantiated from those parameters.\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m     to_remove \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/utils/quantization_config.py:831\u001b[0m, in \u001b[0;36mAwqConfig.__init__\u001b[0;34m(self, bits, group_size, zero_point, version, backend, do_fuse, fuse_max_seq_len, modules_to_fuse, modules_to_not_convert, exllama_config, **kwargs)\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_fuse \u001b[38;5;241m=\u001b[39m do_fuse\n\u001b[1;32m    829\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuse_max_seq_len \u001b[38;5;241m=\u001b[39m fuse_max_seq_len\n\u001b[0;32m--> 831\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/utils/quantization_config.py:886\u001b[0m, in \u001b[0;36mAwqConfig.post_init\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    881\u001b[0m         awq_version_supports_non_conversion \u001b[38;5;241m=\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\n\u001b[1;32m    882\u001b[0m             importlib\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mversion(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mautoawq\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    883\u001b[0m         ) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(MIN_AWQ_VERSION)\n\u001b[1;32m    885\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m awq_version_supports_non_conversion:\n\u001b[0;32m--> 886\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    887\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou current version of `autoawq` does not support module quantization skipping, please upgrade `autoawq` package to at least \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMIN_AWQ_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    888\u001b[0m         )\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_fuse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodules_to_fuse \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    891\u001b[0m     required_keys \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    892\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhidden_size\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    893\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_attention_heads\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_alibi\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    899\u001b[0m     ]\n",
      "\u001b[0;31mValueError\u001b[0m: You current version of `autoawq` does not support module quantization skipping, please upgrade `autoawq` package to at least 0.1.8."
     ]
    }
   ],
   "source": [
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "import os\n",
    "import requests\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"../model/Qwen/Qwen2-VL-2B-Instruct-AWQ\", device_map=\"auto\"\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(\"/data/model/Qwen/Qwen2-VL-2B-Instruct-AWQ\")\n",
    "\n",
    "# Image\n",
    "url_1 = \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\"\n",
    "image_1 = Image.open(requests.get(url_1, stream=True).raw)\n",
    "url_2 = \"https://pic2.zhimg.com/v2-284d76d52cc507a0637ee06913aa07bf_1440w.jpg\"\n",
    "image_2 = Image.open(requests.get(url_2, stream=True).raw)\n",
    "\n",
    "image_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n1.<|vision_start|><|image_pad|><|vision_end|>2.<|vision_start|><|image_pad|><|vision_end|>Judge whether the two images are similar or not., You should say 'Yes' or 'No'.<|im_end|>\\n<|im_start|>assistant\\n\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Messages containing multiple images and a text query\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"1.\"},\n",
    "            {\"type\": \"image\"},\n",
    "            {\"type\": \"text\", \"text\": \"2.\"},\n",
    "            {\"type\": \"image\"},\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"Judge whether the two images are similar or not., You should say 'Yes' or 'No'.\",\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Preparation for inference\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 token probabilities: tensor([[0.5830, 0.3647, 0.0048, 0.0040, 0.0031]], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "Top 5 tokens: ['No', 'Yes', 'This', '根据', 'The']\n"
     ]
    }
   ],
   "source": [
    "image_inputs = [image_1, image_2]\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(\"cuda\")\n",
    "\n",
    "# Inference\n",
    "generate_kwargs = {\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.8,\n",
    "}\n",
    "\n",
    "# Generate the next token probabilities\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    next_token_logits = outputs.logits[:, -1, :]\n",
    "    next_token_probs = torch.softmax(next_token_logits, dim=-1)\n",
    "\n",
    "# Get the top 5 tokens with the highest probabilities\n",
    "top_k_probs, top_k_indices = torch.topk(next_token_probs, k=5, dim=-1)\n",
    "print(\"Top 5 token probabilities:\", top_k_probs)\n",
    "# print(\"Top 5 token indices:\", top_k_indices)\n",
    "\n",
    "# decode the top 5 tokens\n",
    "top_k_tokens = processor.batch_decode(top_k_indices.view(-1, 1))\n",
    "print(\"Top 5 tokens:\", top_k_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1336"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs.input_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VLLM to accelerate the inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hutu/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-01-09 14:22:38,113\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-09 14:22:48 config.py:350] This model supports multiple tasks: {'embedding', 'generate'}. Defaulting to 'generate'.\n",
      "WARNING 01-09 14:22:48 config.py:428] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 01-09 14:22:48 config.py:1020] Defaulting to use mp for distributed inference\n",
      "WARNING 01-09 14:22:48 config.py:503] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 01-09 14:22:48 llm_engine.py:249] Initializing an LLM engine (v0.6.4.post1) with config: model='../model/Qwen/Qwen2-VL-2B-Instruct-AWQ', speculative_config=None, tokenizer='../model/Qwen/Qwen2-VL-2B-Instruct-AWQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=5120, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=../model/Qwen/Qwen2-VL-2B-Instruct-AWQ, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, chat_template_text_format=string, mm_processor_kwargs=None, pooler_config=None)\n",
      "WARNING 01-09 14:22:48 multiproc_gpu_executor.py:56] Reducing Torch parallelism from 44 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 01-09 14:22:48 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "INFO 01-09 14:22:49 selector.py:261] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 01-09 14:22:49 selector.py:144] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1022144)\u001b[0;0m INFO 01-09 14:22:49 selector.py:261] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1022144)\u001b[0;0m INFO 01-09 14:22:49 selector.py:144] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1022144)\u001b[0;0m INFO 01-09 14:22:51 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 01-09 14:22:54 utils.py:961] Found nccl from library libnccl.so.2\n",
      "INFO 01-09 14:22:54 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1022144)\u001b[0;0m INFO 01-09 14:22:54 utils.py:961] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1022144)\u001b[0;0m INFO 01-09 14:22:54 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 01-09 14:22:54 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/hutu/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "WARNING 01-09 14:22:54 custom_all_reduce.py:143] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1022144)\u001b[0;0m INFO 01-09 14:22:54 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/hutu/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1022144)\u001b[0;0m WARNING 01-09 14:22:54 custom_all_reduce.py:143] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "INFO 01-09 14:22:54 shm_broadcast.py:236] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7f1567f1ff20>, local_subscribe_port=52889, remote_subscribe_port=None)\n",
      "INFO 01-09 14:22:54 model_runner.py:1072] Starting to load model ../model/Qwen/Qwen2-VL-2B-Instruct-AWQ...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1022144)\u001b[0;0m INFO 01-09 14:22:54 model_runner.py:1072] Starting to load model ../model/Qwen/Qwen2-VL-2B-Instruct-AWQ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.34it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.34it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-09 14:22:55 model_runner.py:1077] Loading model weights took 1.1682 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1022144)\u001b[0;0m INFO 01-09 14:22:55 model_runner.py:1077] Loading model weights took 1.1682 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1022144)\u001b[0;0m INFO 01-09 14:23:00 worker.py:232] Memory profiling results: total_gpu_memory=10.75GiB initial_memory_usage=1.96GiB peak_torch_memory=1.52GiB memory_usage_post_profile=2.43GiB non_torch_memory=1.25GiB kv_cache_size=6.90GiB gpu_memory_utilization=0.90\n",
      "INFO 01-09 14:23:00 worker.py:232] Memory profiling results: total_gpu_memory=10.75GiB initial_memory_usage=1.96GiB peak_torch_memory=1.52GiB memory_usage_post_profile=2.43GiB non_torch_memory=1.25GiB kv_cache_size=6.90GiB gpu_memory_utilization=0.90\n",
      "INFO 01-09 14:23:01 distributed_gpu_executor.py:57] # GPU blocks: 32311, # CPU blocks: 18724\n",
      "INFO 01-09 14:23:01 distributed_gpu_executor.py:61] Maximum concurrency for 5120 tokens per request: 100.97x\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<vllm.entrypoints.llm.LLM at 0x7f13e9f9c950>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoProcessor\n",
    "import vllm\n",
    "from logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor\n",
    "import os\n",
    "\n",
    "model_path = \"../model/Qwen/Qwen2-VL-2B-Instruct-AWQ\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "processor = AutoProcessor.from_pretrained(model_path)\n",
    "\n",
    "llm = vllm.LLM(\n",
    "    model_path,\n",
    "    quantization=\"awq\",\n",
    "    tensor_parallel_size=2,\n",
    "    dtype=\"half\",\n",
    "    max_model_len=5120,\n",
    "    disable_log_stats=True,\n",
    "    enforce_eager=True,\n",
    ")\n",
    "\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# Image\n",
    "url_1 = \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\"\n",
    "url_2 = \"https://pic2.zhimg.com/v2-284d76d52cc507a0637ee06913aa07bf_1440w.jpg\"\n",
    "\n",
    "# Messages containing multiple images and a text query\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"1.\"},\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": url_1,\n",
    "                },\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"2.\"},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\"url\": url_2}},\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"Judge whether the two images are similar or not., You should say 'Yes' or 'No'.\",\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "messages = str(messages)\n",
    "\n",
    "# Preparation for inference\n",
    "# text = processor.apply_chat_template(\n",
    "#     messages, tokenize=False, add_generation_prompt=True\n",
    "# )\n",
    "\n",
    "# text\n",
    "# texts = [text for _ in range(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\\'role\\': \\'user\\', \\'content\\': [{\\'type\\': \\'text\\', \\'text\\': \\'1.\\'}, {\\'type\\': \\'image_url\\', \\'image_url\\': {\\'url\\': \\'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\\'}}, {\\'type\\': \\'text\\', \\'text\\': \\'2.\\'}, {\\'type\\': \\'image_url\\', \\'image_url\\': {\\'url\\': \\'https://pic2.zhimg.com/v2-284d76d52cc507a0637ee06913aa07bf_1440w.jpg\\'}}, {\\'type\\': \\'text\\', \\'text\\': \"Judge whether the two images are similar or not., You should say \\'Yes\\' or \\'No\\'.\"}]}]'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  5.83it/s, est. speed input: 1009.08 toks/s, output: 5.87 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "logits_processor = MultipleChoiceLogitsProcessor(tokenizer, choices=[\"Yes\", \"No\"])\n",
    "\n",
    "response = llm.generate(\n",
    "    [messages],\n",
    "    vllm.SamplingParams(\n",
    "        n=1,\n",
    "        top_k=1,\n",
    "        temperature=0,\n",
    "        max_tokens=1,\n",
    "        logits_processors=[logits_processor],\n",
    "    ),\n",
    "    use_tqdm=True,\n",
    ")\n",
    "\n",
    "for resp in response:\n",
    "    print(resp.outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
