{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-12T10:43:20.980177Z",
     "iopub.status.busy": "2024-12-12T10:43:20.979918Z",
     "iopub.status.idle": "2024-12-12T10:43:38.821493Z",
     "shell.execute_reply": "2024-12-12T10:43:38.820813Z",
     "shell.execute_reply.started": "2024-12-12T10:43:20.980147Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hutu/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.47.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "comp_prefix = \"data\"\n",
    "train_path = comp_prefix + \"/train.csv\"\n",
    "test_path = comp_prefix + \"/test.csv\"\n",
    "\n",
    "CV = True if len(pd.read_csv(test_path)) < 10 else False\n",
    "\n",
    "img_dir = \"data/train_images\" if CV else \"data/test_images\"\n",
    "split = \"valid\"\n",
    "\n",
    "use_CLIP = False\n",
    "use_BGE = False\n",
    "use_VisBGE = True\n",
    "\n",
    "model2path = {\n",
    "    \"clip\": \"modeloutput/1226/infonce/image/checkpoint-1750\",\n",
    "    \"bge\": \"model/bge_base_en_v1.5\",\n",
    "    \"reranker\": \"model/bge_reranker_base\",\n",
    "    \"visbge\": \"model/BAAI/bge-visualized/Visualized_m3.pth\",\n",
    "    \"marqo\": \"/ssd2/hutu_data/MMRetrieval/model/Marqo/marqo-ecommerce-embeddings-L\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils function and Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-12T10:43:38.823625Z",
     "iopub.status.busy": "2024-12-12T10:43:38.823132Z",
     "iopub.status.idle": "2024-12-12T10:43:38.836848Z",
     "shell.execute_reply": "2024-12-12T10:43:38.836041Z",
     "shell.execute_reply.started": "2024-12-12T10:43:38.823597Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def visualize(df, index, col):\n",
    "    \"\"\"可视化图片和预测结果，预测结果为posting_id的list\"\"\"\n",
    "    row = df.iloc[index]\n",
    "    preds = row[col]\n",
    "    img_dir: str = \"data/train_images\"\n",
    "    images = [df[df.posting_id == pred].image.values[0] for pred in preds]\n",
    "\n",
    "    target_title = row.title\n",
    "    target_img = row.image\n",
    "\n",
    "    titles = [df[df.posting_id == pred].title.values[0] for pred in preds]\n",
    "    images = [Image.open(os.path.join(img_dir, img)) for img in images]\n",
    "\n",
    "    rows = 5\n",
    "    cols = 5\n",
    "    fig, ax = plt.subplots(rows, cols, figsize=(20, 20))\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    for i in range(cols):\n",
    "        ax[0, i].axis(\"off\")\n",
    "\n",
    "    for i in range(rows):\n",
    "        ax[i, 0].axis(\"off\")\n",
    "\n",
    "    ax[0, 0].imshow(Image.open(os.path.join(img_dir, target_img)))\n",
    "    ax[0, 0].set_title(\n",
    "        \"\\n\".join([target_title[i : i + 10] for i in range(0, len(target_title), 10)]),\n",
    "        fontsize=12,\n",
    "    )\n",
    "\n",
    "    for i in range(1, rows):\n",
    "        for j in range(1, cols):\n",
    "            idx = (i - 1) * cols + (j - 1)\n",
    "            ax[i, j].axis(\"off\")\n",
    "            if idx < len(images):\n",
    "                ax[i, j].imshow(images[idx])\n",
    "                ax[i, j].set_title(\n",
    "                    \"\\n\".join(\n",
    "                        [\n",
    "                            titles[idx][k : k + 10]\n",
    "                            for k in range(0, len(titles[idx]), 10)\n",
    "                        ]\n",
    "                    ),\n",
    "                    fontsize=12,\n",
    "                )\n",
    "\n",
    "\n",
    "def compute_f1(col):\n",
    "    def f1(row):\n",
    "        n = len(np.intersect1d(row[\"label\"], row[col]))\n",
    "        return 2 * n / (len(row[\"label\"]) + len(row[col]))\n",
    "\n",
    "    return f1\n",
    "\n",
    "\n",
    "def compute_recall(col):\n",
    "    def recall(row):\n",
    "        n = len(np.intersect1d(row[\"label\"], row[col]))\n",
    "        return n / len(row[\"label\"])\n",
    "\n",
    "    return recall\n",
    "\n",
    "\n",
    "def compute_precision(col):\n",
    "    def precision(row):\n",
    "        n = len(np.intersect1d(row[\"label\"], row[col]))\n",
    "        if len(row[col]) == 0:\n",
    "            return 0\n",
    "        return n / len(row[col])\n",
    "\n",
    "    return precision\n",
    "\n",
    "\n",
    "def compute_AP(col, N):\n",
    "    \"\"\"compute average precision\"\"\"\n",
    "\n",
    "    def AP(row):\n",
    "        K = N\n",
    "\n",
    "        if len(row[col]) == 0:\n",
    "            return 0.0\n",
    "\n",
    "        if len(row[col]) > N:\n",
    "            pred = row[col][:N]\n",
    "        else:\n",
    "            K = len(row[col])\n",
    "            pred = row[col]\n",
    "\n",
    "        actual = row[\"label\"]\n",
    "        score = 0.0\n",
    "        num_hits = 0.0\n",
    "\n",
    "        for i, p in enumerate(pred):\n",
    "            if p in actual and p not in pred[:i]:\n",
    "                num_hits += 1.0\n",
    "                score += num_hits / (i + 1.0)\n",
    "\n",
    "        return score / min(len(actual), K)\n",
    "\n",
    "    return AP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T10:43:38.838322Z",
     "iopub.status.busy": "2024-12-12T10:43:38.837997Z",
     "iopub.status.idle": "2024-12-12T10:43:38.863566Z",
     "shell.execute_reply": "2024-12-12T10:43:38.862792Z",
     "shell.execute_reply.started": "2024-12-12T10:43:38.838287Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ShopeeDataset(Dataset):\n",
    "    def __init__(self, df, img_dir, split: str = \"train\"):\n",
    "        self.df = df\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(512),\n",
    "                transforms.Lambda(lambda img: img.convert(\"RGB\")),\n",
    "                transforms.ToTensor(),\n",
    "            ]\n",
    "        )\n",
    "        self.split = split\n",
    "        self.len = len(self.df)\n",
    "        # self.imgs = self._read_all_images()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        if self.split == \"train\":\n",
    "            query_text = row[\"query\"]\n",
    "            pos_text = row[\"pos_txt\"][0]\n",
    "            neg_text = row[\"neg_txt\"][0]\n",
    "\n",
    "            query_img_path = os.path.join(self.img_dir, row[\"image\"])\n",
    "            pos_img_path = os.path.join(self.img_dir, row[\"pos_img\"][0])\n",
    "            neg_img_path = os.path.join(self.img_dir, row[\"neg_img\"][0])\n",
    "\n",
    "            query_img = self._get_image(query_img_path)\n",
    "            pos_img = self._get_image(pos_img_path)\n",
    "            neg_img = self._get_image(neg_img_path)\n",
    "\n",
    "            return {\n",
    "                \"query\": {\n",
    "                    \"text\": query_text,\n",
    "                    \"image\": query_img,\n",
    "                },\n",
    "                \"pos\": {\n",
    "                    \"text\": pos_text,\n",
    "                    \"image\": pos_img,\n",
    "                },\n",
    "                \"neg\": {\n",
    "                    \"text\": neg_text,\n",
    "                    \"image\": neg_img,\n",
    "                },\n",
    "            }\n",
    "\n",
    "        elif self.split == \"valid\":\n",
    "            title = row[\"title\"]\n",
    "            image_path = os.path.join(self.img_dir, row[\"image\"])\n",
    "\n",
    "            img = self._get_image(image_path)\n",
    "            pil_img = Image.open(image_path)\n",
    "\n",
    "            return {\n",
    "                \"title\": title,\n",
    "                \"image\": img,\n",
    "                \"pil_image\": pil_img,\n",
    "                \"image_path\": image_path,\n",
    "            }\n",
    "\n",
    "    def _get_image(self, path):\n",
    "        img = Image.open(path)\n",
    "        img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "    def get_pil_image(self, path):\n",
    "        return Image.open(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T10:43:38.865637Z",
     "iopub.status.busy": "2024-12-12T10:43:38.865357Z",
     "iopub.status.idle": "2024-12-12T10:43:38.881523Z",
     "shell.execute_reply": "2024-12-12T10:43:38.880762Z",
     "shell.execute_reply.started": "2024-12-12T10:43:38.865606Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "\n",
    "def retrieval(embs, df, chunk_size=4096, threshold=None, topK=None):\n",
    "    assert (\n",
    "        threshold is not None or topK is not None\n",
    "    ), \"Either threshold or topK should be provided\"\n",
    "    assert (\n",
    "        threshold is None or topK is None\n",
    "    ), \"Only one of threshold or topK should be provided\"\n",
    "\n",
    "    embs_pt = torch.tensor(embs).cuda()\n",
    "\n",
    "    num_chunks = (embs_pt.shape[0] + chunk_size - 1) // chunk_size\n",
    "    posting_id = df.posting_id.to_list()\n",
    "    topk_posting_id = []\n",
    "\n",
    "    print(f\"Chunk Size: {chunk_size}, {num_chunks} chunks\")\n",
    "\n",
    "    for i in tqdm(range(num_chunks)):\n",
    "        start = i * chunk_size\n",
    "        end = min((i + 1) * chunk_size, embs_pt.shape[0])\n",
    "        sim = embs_pt[start:end] @ embs_pt.T\n",
    "\n",
    "        if topK is not None:\n",
    "            indices = torch.topk(sim, topK, dim=1).indices.cpu().numpy()\n",
    "            topk_posting_id.extend([[posting_id[j] for j in row] for row in indices])\n",
    "        elif threshold is not None:\n",
    "            mask = sim > threshold\n",
    "            indices = [\n",
    "                torch.nonzero(mask[j]).squeeze().cpu().numpy()\n",
    "                for j in range(mask.shape[0])\n",
    "            ]\n",
    "            indices = [np.unique(i) for i in indices]\n",
    "            sorted_indices = [\n",
    "                indices[j][np.argsort(-sim[j, indices[j]].cpu().numpy())]\n",
    "                for j in range(len(indices))\n",
    "            ]\n",
    "            topk_posting_id.extend(\n",
    "                [[posting_id[j] for j in row] for row in sorted_indices]\n",
    "            )\n",
    "\n",
    "    # clean up\n",
    "    del embs_pt\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return topk_posting_id\n",
    "\n",
    "\n",
    "def rerank(queries, preds, df, model, tokenizer, threshold=-0.1):\n",
    "    # 多GPU并行处理\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    chunk_size = len(queries) // num_gpus\n",
    "\n",
    "    # copy model\n",
    "    models = []\n",
    "    for i in range(num_gpus):\n",
    "        model_i = model.__class__.from_pretrained(model.config._name_or_path)\n",
    "        model_i.cuda(i)\n",
    "        model_i.load_state_dict(model.state_dict())\n",
    "        model_i.eval()\n",
    "        models.append(model_i)\n",
    "    del model\n",
    "\n",
    "    def process_chunk(start_idx, end_idx, model):\n",
    "        chunk = 3\n",
    "        for i in tqdm(range(start_idx, end_idx, chunk)):\n",
    "            chunk_start = i\n",
    "            chunk_end = min(i + chunk, end_idx)\n",
    "\n",
    "            query_chunk = queries[chunk_start:chunk_end]\n",
    "            pred_chunk = preds[chunk_start:chunk_end]\n",
    "\n",
    "            # 根据pred，pred代表的是posting_id，找到对应的title\n",
    "            pred_text_chunk = [\n",
    "                [df[df.posting_id == p].title.values[0] for p in pred]\n",
    "                for pred in pred_chunk\n",
    "            ]\n",
    "\n",
    "            pairs = [\n",
    "                [[q, t] for t in pt] for q, pt in zip(query_chunk, pred_text_chunk)\n",
    "            ]\n",
    "\n",
    "            K = len(pairs[0])\n",
    "\n",
    "            # squeeze pairs\n",
    "            pairs = [p for pair in pairs for p in pair]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                inputs = tokenizer(\n",
    "                    pairs,\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    return_tensors=\"pt\",\n",
    "                    max_length=512,\n",
    "                )\n",
    "                device = model.device\n",
    "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "                scores = (\n",
    "                    model(**inputs, return_dict=True)\n",
    "                    .logits.view(\n",
    "                        -1,\n",
    "                    )\n",
    "                    .float()\n",
    "                )\n",
    "                mask = scores > threshold\n",
    "                mask = mask.view(-1, K)\n",
    "                mask = mask.cpu().numpy()\n",
    "                for i, pred in enumerate(pred_chunk):\n",
    "                    preds[i] = [p for p, m in zip(pred, mask[i]) if m]\n",
    "\n",
    "    threads = []\n",
    "\n",
    "    for i in range(num_gpus):\n",
    "        start_idx = i * chunk_size\n",
    "        end_idx = min((i + 1) * chunk_size, len(queries))\n",
    "        thread = threading.Thread(\n",
    "            target=process_chunk, args=(start_idx, end_idx, models[i])\n",
    "        )\n",
    "        threads.append(thread)\n",
    "        thread.start()\n",
    "\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "    return preds\n",
    "\n",
    "\n",
    "def neighbor_blend(embs, threshold, chunk_size=4096):\n",
    "    embs_pt = torch.tensor(embs).cuda()\n",
    "    num_chunks = (embs_pt.shape[0] + chunk_size - 1) // chunk_size\n",
    "    res_embs = []\n",
    "\n",
    "    print(f\"Chunk Size: {chunk_size}, {num_chunks} chunks\")\n",
    "\n",
    "    for i in tqdm(range(num_chunks)):\n",
    "        start = i * chunk_size\n",
    "        end = min((i + 1) * chunk_size, embs_pt.shape[0])\n",
    "        sim = embs_pt[start:end] @ embs_pt.T\n",
    "        mask = sim > threshold\n",
    "\n",
    "        indices = [\n",
    "            torch.nonzero(mask[j]).squeeze().cpu().numpy() for j in range(mask.shape[0])\n",
    "        ]\n",
    "        indices = [np.unique(i) for i in indices]\n",
    "\n",
    "        # blend embs according to the similarity\n",
    "        for j in range(len(indices)):\n",
    "            weights = sim[j, indices[j]]\n",
    "            res_embs.append(\n",
    "                (weights.unsqueeze(1) * embs_pt[indices[j]]).sum(dim=0).cpu().numpy()\n",
    "            )\n",
    "\n",
    "    # clean up\n",
    "    del embs_pt\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    res_embs = np.stack(res_embs)\n",
    "    res_embs = res_embs / np.linalg.norm(res_embs, axis=1, keepdims=True)\n",
    "\n",
    "    return res_embs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load csv to df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T10:43:39.068700Z",
     "iopub.status.busy": "2024-12-12T10:43:39.068458Z",
     "iopub.status.idle": "2024-12-12T10:43:39.085903Z",
     "shell.execute_reply": "2024-12-12T10:43:39.085066Z",
     "shell.execute_reply.started": "2024-12-12T10:43:39.068676Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posting_id</th>\n",
       "      <th>image</th>\n",
       "      <th>image_phash</th>\n",
       "      <th>title</th>\n",
       "      <th>label_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_129225211</td>\n",
       "      <td>0000a68812bc7e98c42888dfb1c07da0.jpg</td>\n",
       "      <td>94974f937d4c2433</td>\n",
       "      <td>Paper Bag Victoria Secret</td>\n",
       "      <td>249114794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_3386243561</td>\n",
       "      <td>00039780dfc94d01db8676fe789ecd05.jpg</td>\n",
       "      <td>af3f9460c2838f0f</td>\n",
       "      <td>Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...</td>\n",
       "      <td>2937985045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         posting_id                                 image       image_phash  \\\n",
       "0   train_129225211  0000a68812bc7e98c42888dfb1c07da0.jpg  94974f937d4c2433   \n",
       "1  train_3386243561  00039780dfc94d01db8676fe789ecd05.jpg  af3f9460c2838f0f   \n",
       "\n",
       "                                               title  label_group  \n",
       "0                          Paper Bag Victoria Secret    249114794  \n",
       "1  Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...   2937985045  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(train_path) if CV else pd.read_csv(test_path)\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T10:43:39.087204Z",
     "iopub.status.busy": "2024-12-12T10:43:39.086881Z",
     "iopub.status.idle": "2024-12-12T10:43:40.531513Z",
     "shell.execute_reply": "2024-12-12T10:43:40.530646Z",
     "shell.execute_reply.started": "2024-12-12T10:43:39.087178Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posting_id</th>\n",
       "      <th>image</th>\n",
       "      <th>image_phash</th>\n",
       "      <th>title</th>\n",
       "      <th>label_group</th>\n",
       "      <th>label</th>\n",
       "      <th>phash</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_129225211</td>\n",
       "      <td>0000a68812bc7e98c42888dfb1c07da0.jpg</td>\n",
       "      <td>94974f937d4c2433</td>\n",
       "      <td>Paper Bag Victoria Secret</td>\n",
       "      <td>249114794</td>\n",
       "      <td>[train_129225211, train_2278313361]</td>\n",
       "      <td>[train_129225211]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_3386243561</td>\n",
       "      <td>00039780dfc94d01db8676fe789ecd05.jpg</td>\n",
       "      <td>af3f9460c2838f0f</td>\n",
       "      <td>Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...</td>\n",
       "      <td>2937985045</td>\n",
       "      <td>[train_3386243561, train_3423213080]</td>\n",
       "      <td>[train_3386243561]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         posting_id                                 image       image_phash  \\\n",
       "0   train_129225211  0000a68812bc7e98c42888dfb1c07da0.jpg  94974f937d4c2433   \n",
       "1  train_3386243561  00039780dfc94d01db8676fe789ecd05.jpg  af3f9460c2838f0f   \n",
       "\n",
       "                                               title  label_group  \\\n",
       "0                          Paper Bag Victoria Secret    249114794   \n",
       "1  Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...   2937985045   \n",
       "\n",
       "                                  label               phash  \n",
       "0   [train_129225211, train_2278313361]   [train_129225211]  \n",
       "1  [train_3386243561, train_3423213080]  [train_3386243561]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if CV:\n",
    "    tmp = df.groupby(\"label_group\").posting_id.agg(\"unique\").to_dict()\n",
    "    df[\"label\"] = df.label_group.map(tmp)\n",
    "\n",
    "tmp = df.groupby(\"image_phash\").posting_id.agg(\"unique\").to_dict()\n",
    "df[\"phash\"] = df.image_phash.map(tmp)\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T10:43:40.533230Z",
     "iopub.status.busy": "2024-12-12T10:43:40.532831Z",
     "iopub.status.idle": "2024-12-12T10:43:47.037747Z",
     "shell.execute_reply": "2024-12-12T10:43:47.036897Z",
     "shell.execute_reply.started": "2024-12-12T10:43:40.533187Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5530933399168149 0.4221722749534632 0.9940677878488098 0.9928263077503293\n"
     ]
    }
   ],
   "source": [
    "if CV:\n",
    "    df[\"f1\"] = df.apply(compute_f1(\"phash\"), axis=1)\n",
    "    df[\"recall\"] = df.apply(compute_recall(\"phash\"), axis=1)\n",
    "    df[\"precision\"] = df.apply(compute_precision(\"phash\"), axis=1)\n",
    "    df[\"AP\"] = df.apply(compute_AP(\"phash\", 50), axis=1)\n",
    "\n",
    "    print(df.f1.mean(), df.recall.mean(), df.precision.mean(), df.AP.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLIP Model linear probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T10:43:47.041240Z",
     "iopub.status.busy": "2024-12-12T10:43:47.040969Z",
     "iopub.status.idle": "2024-12-12T10:43:47.052469Z",
     "shell.execute_reply": "2024-12-12T10:43:47.051832Z",
     "shell.execute_reply.started": "2024-12-12T10:43:47.041212Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CLIPForEmbedding(CLIPModel):\n",
    "    def forward(\n",
    "        self,\n",
    "        inputs,\n",
    "    ):\n",
    "        text_inputs = inputs[\"text\"]\n",
    "        image_inputs = inputs[\"image\"]\n",
    "\n",
    "        text_embs = self.get_text_features(**text_inputs)\n",
    "        image_embs = self.get_image_features(**image_inputs)\n",
    "\n",
    "        return {\"text\": text_embs, \"image\": image_embs}\n",
    "\n",
    "\n",
    "class CLIPForFusion(nn.Module):\n",
    "    def __init__(self, clip_model=None, processor=None):\n",
    "        super().__init__()\n",
    "        self.clip_model = clip_model\n",
    "        self.processor = processor\n",
    "\n",
    "        self.txt_hidden_dim = clip_model.projection_dim\n",
    "        self.img_hidden_dim = clip_model.projection_dim\n",
    "        self.fusion_dim = 512\n",
    "\n",
    "        self.text_fc = nn.Linear(self.txt_hidden_dim, self.fusion_dim)\n",
    "        self.image_fc = nn.Linear(self.img_hidden_dim, self.fusion_dim)\n",
    "        self.act_fn = nn.ReLU()\n",
    "\n",
    "        self.fusion_fc = nn.Linear(self.fusion_dim * 2, self.fusion_dim)\n",
    "\n",
    "        self.projector = nn.Linear(self.fusion_dim, self.fusion_dim)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_name_or_path):\n",
    "        clip_model = CLIPForEmbedding.from_pretrained(model_name_or_path)\n",
    "        processor = CLIPProcessor.from_pretrained(model_name_or_path)\n",
    "        model = cls(clip_model, processor)\n",
    "        model_path = os.path.join(model_name_or_path, \"model.pt\")\n",
    "\n",
    "        if os.path.exists(model_path):\n",
    "            logging.info(f\"Load model.pt from {model_path}\")\n",
    "            state_dict = torch.load(model_path, weights_only=True)\n",
    "            # only load the fc layers\n",
    "            missing_keys, unexpected_keys = model.load_state_dict(\n",
    "                state_dict, strict=False\n",
    "            )\n",
    "            logging.info(f\"missing keys: {missing_keys}\")\n",
    "            logging.info(f\"unexpected keys: {unexpected_keys}\")\n",
    "        else:\n",
    "            logging.warning(\n",
    "                f\"model.pt not found in {model_name_or_path}, initialize from scratch\"\n",
    "            )\n",
    "        return model\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embed = self.clip_model(inputs)\n",
    "\n",
    "        text_embs = embed[\"text\"]\n",
    "        image_embs = embed[\"image\"]\n",
    "\n",
    "        text_fusion = self.act_fn(self.text_fc(text_embs))\n",
    "        image_fusion = self.act_fn(self.image_fc(image_embs))\n",
    "\n",
    "        fusion = self.act_fn(\n",
    "            self.fusion_fc(torch.cat([text_fusion, image_fusion], dim=-1))\n",
    "        )\n",
    "\n",
    "        proj = self.act_fn(self.projector(fusion))\n",
    "\n",
    "        return proj\n",
    "\n",
    "    def encode(self, inputs):\n",
    "        embed = self.clip_model(inputs)\n",
    "        text_embs = embed[\"text\"]\n",
    "        image_embs = embed[\"image\"]\n",
    "\n",
    "        text_fusion = self.act_fn(self.text_fc(text_embs))\n",
    "        image_fusion = self.act_fn(self.image_fc(image_embs))\n",
    "\n",
    "        fusion = self.act_fn(\n",
    "            self.fusion_fc(torch.cat([text_fusion, image_fusion], dim=-1))\n",
    "        )\n",
    "\n",
    "        return fusion\n",
    "\n",
    "\n",
    "def get_collate_fn(processor):\n",
    "    def collate_fn(batch):\n",
    "        texts = [item[\"title\"] for item in batch]\n",
    "        images = [item[\"image\"] for item in batch]\n",
    "        text_encoded = processor(\n",
    "            text=texts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=77,\n",
    "        )\n",
    "        img_encoded = processor(images=images, return_tensors=\"pt\", do_rescale=False)\n",
    "\n",
    "        return {\"text\": text_encoded, \"image\": img_encoded}\n",
    "\n",
    "    return collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T10:43:47.053548Z",
     "iopub.status.busy": "2024-12-12T10:43:47.053320Z",
     "iopub.status.idle": "2024-12-12T10:43:49.337791Z",
     "shell.execute_reply": "2024-12-12T10:43:49.337074Z",
     "shell.execute_reply.started": "2024-12-12T10:43:47.053525Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Incorrect path_or_model_id: 'modeloutput/1226/infonce/image/checkpoint-1750'. Please provide either the path to a local folder or the repo_id of a model on the Hub.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/utils/hub.py:403\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 403\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:106\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 106\u001b[0m     \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:154\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m repo_id\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must be in the form \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnamespace/repo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Use `repo_type` argument if needed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m     )\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n",
      "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': 'modeloutput/1226/infonce/image/checkpoint-1750'. Use `repo_type` argument if needed.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m model_path \u001b[38;5;241m=\u001b[39m model2path[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclip\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# model = CLIPForEmbedding.from_pretrained(model_path)\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mCLIPForFusion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m processor \u001b[38;5;241m=\u001b[39m CLIPProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_path)\n\u001b[1;32m      8\u001b[0m dataset \u001b[38;5;241m=\u001b[39m ShopeeDataset(df, img_dir, split)\n",
      "Cell \u001b[0;32mIn[10], line 35\u001b[0m, in \u001b[0;36mCLIPForFusion.from_pretrained\u001b[0;34m(cls, model_name_or_path)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_pretrained\u001b[39m(\u001b[38;5;28mcls\u001b[39m, model_name_or_path):\n\u001b[0;32m---> 35\u001b[0m     clip_model \u001b[38;5;241m=\u001b[39m \u001b[43mCLIPForEmbedding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     processor \u001b[38;5;241m=\u001b[39m CLIPProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name_or_path)\n\u001b[1;32m     37\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(clip_model, processor)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/modeling_utils.py:3518\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m commit_hash \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3516\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[1;32m   3517\u001b[0m         \u001b[38;5;66;03m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[39;00m\n\u001b[0;32m-> 3518\u001b[0m         resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3519\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3520\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3521\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3522\u001b[0m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3523\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3524\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3525\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3526\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3527\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3528\u001b[0m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3529\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3530\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3531\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3532\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3533\u001b[0m         commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m   3534\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/utils/hub.py:469\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere was a specific connection error when trying to load \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HFValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 469\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    470\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncorrect path_or_model_id: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Please provide either the path to a local folder or the repo_id of a model on the Hub.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    471\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resolved_file\n",
      "\u001b[0;31mOSError\u001b[0m: Incorrect path_or_model_id: 'modeloutput/1226/infonce/image/checkpoint-1750'. Please provide either the path to a local folder or the repo_id of a model on the Hub."
     ]
    }
   ],
   "source": [
    "model_path = model2path[\"clip\"]\n",
    "\n",
    "# model = CLIPForEmbedding.from_pretrained(model_path)\n",
    "model = CLIPForFusion.from_pretrained(model_path)\n",
    "processor = CLIPProcessor.from_pretrained(model_path)\n",
    "\n",
    "\n",
    "dataset = ShopeeDataset(df, img_dir, split)\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=256,\n",
    "    num_workers=16,\n",
    "    collate_fn=get_collate_fn(processor),\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T10:43:49.339031Z",
     "iopub.status.busy": "2024-12-12T10:43:49.338755Z",
     "iopub.status.idle": "2024-12-12T10:43:49.346865Z",
     "shell.execute_reply": "2024-12-12T10:43:49.345940Z",
     "shell.execute_reply.started": "2024-12-12T10:43:49.339005Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if use_CLIP:\n",
    "    embs = []\n",
    "    # get image + text embedding into numpy array\n",
    "\n",
    "    model.eval()\n",
    "    model.cuda()\n",
    "    for batch in tqdm(loader):\n",
    "        with torch.no_grad():\n",
    "            with torch.amp.autocast(\"cuda\", dtype=torch.float16):\n",
    "                for k, v in batch.items():\n",
    "                    batch[k] = {key: val.cuda() for key, val in v.items()}\n",
    "                emb = model(batch)\n",
    "                # text_emb, image_emb = emb[\"text\"], emb[\"image\"]\n",
    "                # emb = torch.cat([text_emb, image_emb], dim=-1)\n",
    "            embs.append(emb.cpu().detach().numpy())\n",
    "\n",
    "    print(len(embs))\n",
    "    embs = np.concatenate(embs, axis=0)\n",
    "    # normalize\n",
    "    embs /= np.linalg.norm(embs, axis=1, keepdims=True)\n",
    "\n",
    "del model, processor\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_CLIP:\n",
    "    # for i in range(3):\n",
    "    #     embs = neighbor_blend(embs, 0.9)\n",
    "    # candidate = np.arange(0.85, 0.95, 0.005)\n",
    "    # for threshold in candidate:\n",
    "    threshold = 0.8\n",
    "    df[\"clip\"] = retrieval(embs, df, threshold=threshold)\n",
    "\n",
    "    if CV:\n",
    "        df[\"f1_clip\"] = df.apply(compute_f1(\"clip\"), axis=1)\n",
    "        df[\"recall_clip\"] = df.apply(compute_recall(\"clip\"), axis=1)\n",
    "        df[\"precision_clip\"] = df.apply(compute_precision(\"clip\"), axis=1)\n",
    "        df[\"AP_clip\"] = df.apply(compute_AP(\"clip\", 50), axis=1)\n",
    "\n",
    "        print(\n",
    "            threshold,\n",
    "            df.f1_clip.mean(),\n",
    "            df.recall_clip.mean(),\n",
    "            df.precision_clip.mean(),\n",
    "            df.AP_clip.mean(),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the pred where f1 is below 0.6\n",
    "if use_CLIP and CV:\n",
    "    index = 2\n",
    "    samples = df[df[\"f1_clip\"] < 0.4].index.to_list()\n",
    "    print(df.iloc[samples[index]][\"f1_clip\"])\n",
    "    visualize(df, samples[index], \"clip\")\n",
    "    visualize(df, samples[index], \"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T10:43:49.348185Z",
     "iopub.status.busy": "2024-12-12T10:43:49.347935Z",
     "iopub.status.idle": "2024-12-12T10:43:50.070465Z",
     "shell.execute_reply": "2024-12-12T10:43:50.069225Z",
     "shell.execute_reply.started": "2024-12-12T10:43:49.348161Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "model_path = model2path[\"bge\"]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModel.from_pretrained(model_path)\n",
    "\n",
    "dataset = ShopeeDataset(df, img_dir, split)\n",
    "\n",
    "\n",
    "def get_collate_fn():\n",
    "    def collate_fn(batch):\n",
    "        texts = [item[\"title\"] for item in batch]\n",
    "        return texts\n",
    "\n",
    "    return collate_fn\n",
    "\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=256,\n",
    "    num_workers=16,\n",
    "    collate_fn=get_collate_fn(),\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T10:43:50.072521Z",
     "iopub.status.busy": "2024-12-12T10:43:50.071832Z",
     "iopub.status.idle": "2024-12-12T10:46:59.779784Z",
     "shell.execute_reply": "2024-12-12T10:46:59.778686Z",
     "shell.execute_reply.started": "2024-12-12T10:43:50.072480Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if use_BGE:\n",
    "    embs = []\n",
    "    # get text embedding into numpy array\n",
    "\n",
    "    model.eval()\n",
    "    model.cuda()\n",
    "    for batch in tqdm(loader):\n",
    "        with torch.no_grad():\n",
    "            inputs = tokenizer(\n",
    "                batch, return_tensors=\"pt\", padding=True, truncation=True\n",
    "            )\n",
    "            with torch.amp.autocast(\"cuda\", dtype=torch.float16):\n",
    "                # print(inputs)\n",
    "                inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "                output = model(**inputs)\n",
    "                emb = output.last_hidden_state[:, 0]\n",
    "            embs.append(emb.cpu().detach().numpy())\n",
    "\n",
    "    print(len(embs))\n",
    "    embs = np.concatenate(embs, axis=0)\n",
    "    # normalize\n",
    "    embs /= np.linalg.norm(embs, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "del model, tokenizer\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "if use_BGE:\n",
    "    df[\"bge\"] = retrieval(embs, df, topK=20)\n",
    "\n",
    "    # use rerank\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model2path[\"reranker\"]\n",
    "    ).cuda()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model2path[\"reranker\"])\n",
    "\n",
    "    queries = df.title.to_list()\n",
    "    preds = df.bge.to_list()\n",
    "\n",
    "    df[\"bge\"] = rerank(queries, preds, df, model, tokenizer, threshold=0)\n",
    "\n",
    "    if CV:\n",
    "        df[\"f1_bge\"] = df.apply(compute_f1(\"bge\"), axis=1)\n",
    "        df[\"recall_bge\"] = df.apply(compute_recall(\"bge\"), axis=1)\n",
    "        df[\"precision_bge\"] = df.apply(compute_precision(\"bge\"), axis=1)\n",
    "        df[\"AP_bge\"] = df.apply(compute_AP(\"bge\", 50), axis=1)\n",
    "\n",
    "        print(\n",
    "            df.f1_bge.mean(),\n",
    "            df.recall_bge.mean(),\n",
    "            df.precision_bge.mean(),\n",
    "            df.AP_bge.mean(),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualized-BGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_collate_fn(model):\n",
    "    def collate_fn(batch):\n",
    "        texts = [item[\"title\"] for item in batch]\n",
    "        imgs = [item[\"pil_image\"] for item in batch]\n",
    "\n",
    "        texts_encoded = model.tokenizer(\n",
    "            texts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "        )\n",
    "        imgs_encoded = [model.preprocess_val(img) for img in imgs]\n",
    "\n",
    "        imgs_encoded = torch.stack(imgs_encoded)\n",
    "\n",
    "        return {\n",
    "            \"text\": texts_encoded,\n",
    "            \"img\": imgs_encoded,\n",
    "        }\n",
    "\n",
    "    return collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hutu/miniconda3/lib/python3.12/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "/ssd2/hutu_data/MMRetrieval/train/modeling_visbge.py:115: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(model_weight, map_location=\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "from train.modeling_visbge import Visualized_BGE\n",
    "\n",
    "model = Visualized_BGE(model_name_bge=\"BAAI/bge-m3\", model_weight=model2path[\"visbge\"])\n",
    "\n",
    "dataset = ShopeeDataset(df, img_dir, split)\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=128,\n",
    "    num_workers=16,\n",
    "    collate_fn=get_collate_fn(model),\n",
    "    shuffle=False,\n",
    ")\n",
    "# iter(loader).__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 268/268 [07:26<00:00,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "268\n"
     ]
    }
   ],
   "source": [
    "if use_VisBGE:\n",
    "    embs = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader):\n",
    "            with torch.amp.autocast(\"cuda\", dtype=torch.float16):\n",
    "                images = batch[\"img\"].cuda()\n",
    "                texts = batch[\"text\"]\n",
    "                for k, v in texts.items():\n",
    "                    texts[k] = v.cuda()\n",
    "                emb = model.encode_mm(images=images, texts=texts)\n",
    "            embs.append(emb.cpu().detach().numpy())\n",
    "\n",
    "    print(len(embs))\n",
    "\n",
    "    embs = np.concatenate(embs, axis=0)\n",
    "\n",
    "    # normalize\n",
    "    embs /= np.linalg.norm(embs, axis=1, keepdims=True)\n",
    "\n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk Size: 4096, 9 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:09<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6804761959768053 0.732974992064425 0.7704290093357701 0.8782147259638239\n"
     ]
    }
   ],
   "source": [
    "if use_VisBGE:\n",
    "    df[\"visbge\"] = retrieval(embs, df, threshold=0.78)\n",
    "    if CV:\n",
    "        df[\"f1_visbge\"] = df.apply(compute_f1(\"visbge\"), axis=1)\n",
    "        df[\"recall_visbge\"] = df.apply(compute_recall(\"visbge\"), axis=1)\n",
    "        df[\"precision_visbge\"] = df.apply(compute_precision(\"visbge\"), axis=1)\n",
    "        df[\"AP_visbge\"] = df.apply(compute_AP(\"visbge\", 50), axis=1)\n",
    "\n",
    "        print(\n",
    "            df.f1_visbge.mean(),\n",
    "            df.recall_visbge.mean(),\n",
    "            df.precision_visbge.mean(),\n",
    "            df.AP_visbge.mean(),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the pred where f1 is below 0.6\n",
    "if use_VisBGE and CV:\n",
    "    index = 6\n",
    "    samples = df[df[\"f1_visbge\"] < 0.6].index.to_list()\n",
    "    visualize(df, samples[index], \"visbge\")\n",
    "    visualize(df, samples[index], \"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T10:46:59.781316Z",
     "iopub.status.busy": "2024-12-12T10:46:59.781041Z",
     "iopub.status.idle": "2024-12-12T10:47:00.651401Z",
     "shell.execute_reply": "2024-12-12T10:47:00.650696Z",
     "shell.execute_reply.started": "2024-12-12T10:46:59.781286Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def combine(cols):\n",
    "    def combine_(row):\n",
    "        return np.unique(np.concatenate([row[col] for col in cols]))\n",
    "\n",
    "    return combine_\n",
    "\n",
    "\n",
    "cols = [\"phash\"]\n",
    "if use_BGE:\n",
    "    cols.append(\"bge\")\n",
    "if use_CLIP:\n",
    "    cols.append(\"clip\")\n",
    "if use_VisBGE:\n",
    "    cols.append(\"visbge\")\n",
    "\n",
    "df[\"matches\"] = df.apply(combine(cols), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T10:47:00.652717Z",
     "iopub.status.busy": "2024-12-12T10:47:00.652444Z",
     "iopub.status.idle": "2024-12-12T10:47:12.630818Z",
     "shell.execute_reply": "2024-12-12T10:47:12.629950Z",
     "shell.execute_reply.started": "2024-12-12T10:47:00.652691Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if CV:\n",
    "    df[\"f1\"] = df.apply(compute_f1(\"matches\"), axis=1)\n",
    "    df[\"recall\"] = df.apply(compute_recall(\"matches\"), axis=1)\n",
    "    df[\"precision\"] = df.apply(compute_precision(\"matches\"), axis=1)\n",
    "    df[\"AP\"] = df.apply(compute_AP(\"matches\", 50), axis=1)\n",
    "\n",
    "    print(\n",
    "        df.f1.mean(),\n",
    "        df.recall.mean(),\n",
    "        df.precision.mean(),\n",
    "        df.AP.mean(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T10:47:12.632177Z",
     "iopub.status.busy": "2024-12-12T10:47:12.631906Z",
     "iopub.status.idle": "2024-12-12T10:47:12.703808Z",
     "shell.execute_reply": "2024-12-12T10:47:12.702909Z",
     "shell.execute_reply.started": "2024-12-12T10:47:12.632151Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "submission = df[[\"posting_id\", \"matches\"]]\n",
    "submission[\"matches\"] = submission[\"matches\"].apply(lambda x: \" \".join(x))\n",
    "submission.columns = [[\"posting_id\", \"matches\"]]\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T10:47:12.705058Z",
     "iopub.status.busy": "2024-12-12T10:47:12.704805Z",
     "iopub.status.idle": "2024-12-12T10:47:12.833496Z",
     "shell.execute_reply": "2024-12-12T10:47:12.832665Z",
     "shell.execute_reply.started": "2024-12-12T10:47:12.705032Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T10:47:12.834807Z",
     "iopub.status.busy": "2024-12-12T10:47:12.834512Z",
     "iopub.status.idle": "2024-12-12T10:47:12.898904Z",
     "shell.execute_reply": "2024-12-12T10:47:12.898072Z",
     "shell.execute_reply.started": "2024-12-12T10:47:12.834755Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv(\"/kaggle/working/submission.csv\")\n",
    "sub"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 1878097,
     "sourceId": 24286,
     "sourceType": "competition"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 1030,
     "modelInstanceId": 3348,
     "sourceId": 4556,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 189204,
     "modelInstanceId": 166883,
     "sourceId": 195722,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 189358,
     "modelInstanceId": 167039,
     "sourceId": 195906,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
