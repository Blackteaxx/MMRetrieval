{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-12T10:43:20.980177Z",
     "iopub.status.busy": "2024-12-12T10:43:20.979918Z",
     "iopub.status.idle": "2024-12-12T10:43:38.821493Z",
     "shell.execute_reply": "2024-12-12T10:43:38.820813Z",
     "shell.execute_reply.started": "2024-12-12T10:43:20.980147Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "comp_prefix = \"/home/hutu/workspace/MMRetrieval/data\"\n",
    "train_path = comp_prefix + \"/train.csv\"\n",
    "test_path = comp_prefix + \"/test.csv\"\n",
    "\n",
    "CV = True if len(pd.read_csv(test_path)) < 10 else False\n",
    "\n",
    "img_dir = \"/home/hutu/workspace/MMRetrieval/data/train_images\" if CV else \"/home/hutu/workspace/MMRetrieval/data/test_images\"\n",
    "split = \"valid\"\n",
    "\n",
    "use_CLIP = False\n",
    "use_BGE = True\n",
    "\n",
    "model2path = {\n",
    "    \"clip\": \"/home/hutu/workspace/MMRetrieval/modeloutput/1211/infoNCE/checkpoint-1080\",\n",
    "    \"bge\": \"/home/hutu/workspace/SentenceEmbedding/modeloutput/1214/checkpoint-2142\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils function and Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-12T10:43:38.823625Z",
     "iopub.status.busy": "2024-12-12T10:43:38.823132Z",
     "iopub.status.idle": "2024-12-12T10:43:38.836848Z",
     "shell.execute_reply": "2024-12-12T10:43:38.836041Z",
     "shell.execute_reply.started": "2024-12-12T10:43:38.823597Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def visualize(df, index, col):\n",
    "    \"\"\"可视化图片和预测结果，预测结果为posting_id的list\"\"\"\n",
    "    row = df.iloc[index]\n",
    "    preds = row[col]\n",
    "    img_dir: str = \"/home/hutu/workspace/MMRetrieval/data/train_images\"\n",
    "    images = [df[df.posting_id == pred].image.values[0] for pred in preds]\n",
    "\n",
    "    target_title = row.title\n",
    "    target_img = row.image\n",
    "\n",
    "    titles = [df[df.posting_id == pred].title.values[0] for pred in preds]\n",
    "    images = [Image.open(os.path.join(img_dir, img)) for img in images]\n",
    "\n",
    "    rows = 5\n",
    "    cols = 5\n",
    "    fig, ax = plt.subplots(rows, cols, figsize=(20, 20))\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    for i in range(cols):\n",
    "        ax[0, i].axis(\"off\")\n",
    "\n",
    "    for i in range(rows):\n",
    "        ax[i, 0].axis(\"off\")\n",
    "\n",
    "    ax[0, 0].imshow(Image.open(os.path.join(img_dir, target_img)))\n",
    "    ax[0, 0].set_title(\n",
    "        \"\\n\".join([target_title[i : i + 10] for i in range(0, len(target_title), 10)]),\n",
    "        fontsize=12,\n",
    "    )\n",
    "\n",
    "    for i in range(1, rows):\n",
    "        for j in range(1, cols):\n",
    "            idx = (i - 1) * cols + (j - 1)\n",
    "            ax[i, j].axis(\"off\")\n",
    "            if idx < len(images):\n",
    "                ax[i, j].imshow(images[idx])\n",
    "                ax[i, j].set_title(\n",
    "                    \"\\n\".join(\n",
    "                        [\n",
    "                            titles[idx][k : k + 10]\n",
    "                            for k in range(0, len(titles[idx]), 10)\n",
    "                        ]\n",
    "                    ),\n",
    "                    fontsize=12,\n",
    "                )\n",
    "\n",
    "\n",
    "def compute_f1(col):\n",
    "    def f1(row):\n",
    "        n = len(np.intersect1d(row[\"label\"], row[col]))\n",
    "        return 2 * n / (len(row[\"label\"]) + len(row[col]))\n",
    "\n",
    "    return f1\n",
    "\n",
    "\n",
    "def compute_recall(col):\n",
    "    def recall(row):\n",
    "        n = len(np.intersect1d(row[\"label\"], row[col]))\n",
    "        return n / len(row[\"label\"])\n",
    "\n",
    "    return recall\n",
    "\n",
    "\n",
    "def compute_precision(col):\n",
    "    def precision(row):\n",
    "        n = len(np.intersect1d(row[\"label\"], row[col]))\n",
    "        return n / len(row[col])\n",
    "\n",
    "    return precision\n",
    "\n",
    "\n",
    "def compute_precision_K(col, K):\n",
    "    def precision(row):\n",
    "        n = len(np.intersect1d(row[\"label\"], row[col][:K]))\n",
    "        return n / K\n",
    "\n",
    "    return precision\n",
    "\n",
    "\n",
    "def compute_AP(col, N):\n",
    "    \"\"\"compute average precision\"\"\"\n",
    "\n",
    "    def AP(row):\n",
    "        n = len(np.intersect1d(row[\"label\"], row[col]))\n",
    "        max_n = min(len(row[col]), N)\n",
    "        if n == 0:\n",
    "            return 0\n",
    "        return (\n",
    "            sum(\n",
    "                [\n",
    "                    compute_precision_K(col, i)(row)\n",
    "                    for i in range(1, max_n + 1)\n",
    "                    if row[col][i - 1] in row[\"label\"]\n",
    "                ]\n",
    "            )\n",
    "            / max_n\n",
    "        )\n",
    "\n",
    "    return AP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T10:43:38.838322Z",
     "iopub.status.busy": "2024-12-12T10:43:38.837997Z",
     "iopub.status.idle": "2024-12-12T10:43:38.863566Z",
     "shell.execute_reply": "2024-12-12T10:43:38.862792Z",
     "shell.execute_reply.started": "2024-12-12T10:43:38.838287Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ShopeeDataset(Dataset):\n",
    "    def __init__(self, df, img_dir, split: str = \"train\"):\n",
    "        self.df = df\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(256),\n",
    "                transforms.CenterCrop(224),\n",
    "                # convert to RGB\n",
    "                transforms.Lambda(lambda img: img.convert(\"RGB\")),\n",
    "                transforms.ToTensor(),\n",
    "            ]\n",
    "        )\n",
    "        self.split = split\n",
    "        self.len = len(self.df)\n",
    "        # self.imgs = self._read_all_images()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        if self.split == \"train\":\n",
    "            query_text = row[\"query\"]\n",
    "            pos_text = row[\"pos_txt\"][0]\n",
    "            neg_text = row[\"neg_txt\"][0]\n",
    "\n",
    "            query_img_path = os.path.join(self.img_dir, row[\"image\"])\n",
    "            pos_img_path = os.path.join(self.img_dir, row[\"pos_img\"][0])\n",
    "            neg_img_path = os.path.join(self.img_dir, row[\"neg_img\"][0])\n",
    "\n",
    "            query_img = self._get_image(query_img_path)\n",
    "            pos_img = self._get_image(pos_img_path)\n",
    "            neg_img = self._get_image(neg_img_path)\n",
    "\n",
    "            return {\n",
    "                \"query\": {\n",
    "                    \"text\": query_text,\n",
    "                    \"image\": query_img,\n",
    "                },\n",
    "                \"pos\": {\n",
    "                    \"text\": pos_text,\n",
    "                    \"image\": pos_img,\n",
    "                },\n",
    "                \"neg\": {\n",
    "                    \"text\": neg_text,\n",
    "                    \"image\": neg_img,\n",
    "                },\n",
    "            }\n",
    "\n",
    "        elif self.split == \"valid\":\n",
    "            title = row[\"title\"]\n",
    "            image_path = os.path.join(self.img_dir, row[\"image\"])\n",
    "\n",
    "            img = self._get_image(image_path)\n",
    "\n",
    "            return {\n",
    "                \"title\": title,\n",
    "                \"image\": img,\n",
    "            }\n",
    "\n",
    "    def _get_image(self, path):\n",
    "        img = Image.open(path)\n",
    "        img = self.transform(img)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T10:43:38.865637Z",
     "iopub.status.busy": "2024-12-12T10:43:38.865357Z",
     "iopub.status.idle": "2024-12-12T10:43:38.881523Z",
     "shell.execute_reply": "2024-12-12T10:43:38.880762Z",
     "shell.execute_reply.started": "2024-12-12T10:43:38.865606Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def retrieval(embs, threshold, df, chunk_size=4096):\n",
    "    embs_pt = torch.tensor(embs).cuda()\n",
    "\n",
    "    num_chunks = (embs_pt.shape[0] + chunk_size - 1) // chunk_size\n",
    "    posting_id = df.posting_id.to_list()\n",
    "    topk_posting_id = []\n",
    "\n",
    "    print(f\"Chunk Size: {chunk_size}, {num_chunks} chunks\")\n",
    "\n",
    "\n",
    "    for i in tqdm(range(num_chunks)):\n",
    "        start = i * chunk_size\n",
    "        end = min((i + 1) * chunk_size, embs_pt.shape[0])\n",
    "        sim = embs_pt[start:end] @ embs_pt.T\n",
    "        mask = sim > threshold\n",
    "\n",
    "        indices = [\n",
    "            torch.nonzero(mask[j]).squeeze().cpu().numpy() for j in range(mask.shape[0])\n",
    "        ]\n",
    "        indices = [np.unique(i) for i in indices]\n",
    "        sorted_indices = [\n",
    "            indices[j][np.argsort(-sim[j, indices[j]].cpu().numpy())]\n",
    "            for j in range(len(indices))\n",
    "        ]\n",
    "\n",
    "        topk_posting_id.extend(\n",
    "            [[posting_id[j] for j in row] for row in sorted_indices]\n",
    "        )\n",
    "\n",
    "    # clean up\n",
    "    del embs_pt\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return topk_posting_id\n",
    "\n",
    "def neighbor_blend(embs, threshold, chunk_size=4096):\n",
    "    embs_pt = torch.tensor(embs).cuda()\n",
    "    num_chunks = (embs_pt.shape[0] + chunk_size - 1) // chunk_size\n",
    "    res_embs = []\n",
    "    \n",
    "    print(f\"Chunk Size: {chunk_size}, {num_chunks} chunks\")\n",
    "\n",
    "    for i in tqdm(range(num_chunks)):\n",
    "        start = i * chunk_size\n",
    "        end = min((i + 1) * chunk_size, embs_pt.shape[0])\n",
    "        sim = embs_pt[start:end] @ embs_pt.T\n",
    "        mask = sim > threshold\n",
    "\n",
    "        indices = [\n",
    "            torch.nonzero(mask[j]).squeeze().cpu().numpy() for j in range(mask.shape[0])\n",
    "        ]\n",
    "        indices = [np.unique(i) for i in indices]\n",
    "        \n",
    "        # blend embs according to the similarity\n",
    "        for j in range(len(indices)):\n",
    "            weights = sim[j, indices[j]].softmax(dim=0)\n",
    "            res_embs.append((weights.unsqueeze(1) * embs_pt[indices[j]]).sum(dim=0).cpu().numpy())\n",
    "        \n",
    "    # clean up\n",
    "    del embs_pt\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    res_embs = np.stack(res_embs)\n",
    "    res_embs = res_embs / np.linalg.norm(res_embs, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "    return res_embs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load csv to df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T10:43:39.068700Z",
     "iopub.status.busy": "2024-12-12T10:43:39.068458Z",
     "iopub.status.idle": "2024-12-12T10:43:39.085903Z",
     "shell.execute_reply": "2024-12-12T10:43:39.085066Z",
     "shell.execute_reply.started": "2024-12-12T10:43:39.068676Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posting_id</th>\n",
       "      <th>image</th>\n",
       "      <th>image_phash</th>\n",
       "      <th>title</th>\n",
       "      <th>label_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_129225211</td>\n",
       "      <td>0000a68812bc7e98c42888dfb1c07da0.jpg</td>\n",
       "      <td>94974f937d4c2433</td>\n",
       "      <td>Paper Bag Victoria Secret</td>\n",
       "      <td>249114794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_3386243561</td>\n",
       "      <td>00039780dfc94d01db8676fe789ecd05.jpg</td>\n",
       "      <td>af3f9460c2838f0f</td>\n",
       "      <td>Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...</td>\n",
       "      <td>2937985045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         posting_id                                 image       image_phash  \\\n",
       "0   train_129225211  0000a68812bc7e98c42888dfb1c07da0.jpg  94974f937d4c2433   \n",
       "1  train_3386243561  00039780dfc94d01db8676fe789ecd05.jpg  af3f9460c2838f0f   \n",
       "\n",
       "                                               title  label_group  \n",
       "0                          Paper Bag Victoria Secret    249114794  \n",
       "1  Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...   2937985045  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(train_path) if CV else pd.read_csv(test_path)\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T10:43:39.087204Z",
     "iopub.status.busy": "2024-12-12T10:43:39.086881Z",
     "iopub.status.idle": "2024-12-12T10:43:40.531513Z",
     "shell.execute_reply": "2024-12-12T10:43:40.530646Z",
     "shell.execute_reply.started": "2024-12-12T10:43:39.087178Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posting_id</th>\n",
       "      <th>image</th>\n",
       "      <th>image_phash</th>\n",
       "      <th>title</th>\n",
       "      <th>label_group</th>\n",
       "      <th>label</th>\n",
       "      <th>phash</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_129225211</td>\n",
       "      <td>0000a68812bc7e98c42888dfb1c07da0.jpg</td>\n",
       "      <td>94974f937d4c2433</td>\n",
       "      <td>Paper Bag Victoria Secret</td>\n",
       "      <td>249114794</td>\n",
       "      <td>[train_129225211, train_2278313361]</td>\n",
       "      <td>[train_129225211]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_3386243561</td>\n",
       "      <td>00039780dfc94d01db8676fe789ecd05.jpg</td>\n",
       "      <td>af3f9460c2838f0f</td>\n",
       "      <td>Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...</td>\n",
       "      <td>2937985045</td>\n",
       "      <td>[train_3386243561, train_3423213080]</td>\n",
       "      <td>[train_3386243561]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         posting_id                                 image       image_phash  \\\n",
       "0   train_129225211  0000a68812bc7e98c42888dfb1c07da0.jpg  94974f937d4c2433   \n",
       "1  train_3386243561  00039780dfc94d01db8676fe789ecd05.jpg  af3f9460c2838f0f   \n",
       "\n",
       "                                               title  label_group  \\\n",
       "0                          Paper Bag Victoria Secret    249114794   \n",
       "1  Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...   2937985045   \n",
       "\n",
       "                                  label               phash  \n",
       "0   [train_129225211, train_2278313361]   [train_129225211]  \n",
       "1  [train_3386243561, train_3423213080]  [train_3386243561]  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if CV:\n",
    "    tmp = df.groupby(\"label_group\").posting_id.agg(\"unique\").to_dict()\n",
    "    df[\"label\"] = df.label_group.map(tmp)\n",
    "\n",
    "tmp = df.groupby(\"image_phash\").posting_id.agg(\"unique\").to_dict()\n",
    "df[\"phash\"] = df.image_phash.map(tmp)\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T10:43:40.533230Z",
     "iopub.status.busy": "2024-12-12T10:43:40.532831Z",
     "iopub.status.idle": "2024-12-12T10:43:47.037747Z",
     "shell.execute_reply": "2024-12-12T10:43:47.036897Z",
     "shell.execute_reply.started": "2024-12-12T10:43:40.533187Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5530933399168149 0.4221722749534632 0.9940677878488098 0.9923105721349715\n"
     ]
    }
   ],
   "source": [
    "if CV:\n",
    "    df[\"f1\"] = df.apply(compute_f1(\"phash\"), axis=1)\n",
    "    df[\"recall\"] = df.apply(compute_recall(\"phash\"), axis=1)\n",
    "    df[\"precision\"] = df.apply(compute_precision(\"phash\"), axis=1)\n",
    "    df[\"AP\"] = df.apply(compute_AP(\"phash\", 50), axis=1)\n",
    "    \n",
    "    print(df.f1.mean(), df.recall.mean(), df.precision.mean(), df.AP.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLIP Model linear probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T10:43:47.041240Z",
     "iopub.status.busy": "2024-12-12T10:43:47.040969Z",
     "iopub.status.idle": "2024-12-12T10:43:47.052469Z",
     "shell.execute_reply": "2024-12-12T10:43:47.051832Z",
     "shell.execute_reply.started": "2024-12-12T10:43:47.041212Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CLIPForEmbedding(CLIPModel):\n",
    "    def forward(\n",
    "        self,\n",
    "        inputs,\n",
    "    ):\n",
    "        text_inputs = inputs[\"text\"]\n",
    "        image_inputs = inputs[\"image\"]\n",
    "\n",
    "        text_embs = self.get_text_features(**text_inputs)\n",
    "        image_embs = self.get_image_features(**image_inputs)\n",
    "\n",
    "        return {\"text\": text_embs, \"image\": image_embs}\n",
    "\n",
    "class CLIPForFusion(nn.Module):\n",
    "    def __init__(self, clip_model=None, processor=None):\n",
    "        super().__init__()\n",
    "        self.clip_model = clip_model\n",
    "        self.processor = processor\n",
    "\n",
    "        self.txt_hidden_dim = 768\n",
    "        self.img_hidden_dim = 768\n",
    "        self.fusion_dim = 512\n",
    "\n",
    "        self.text_fc = nn.Linear(self.txt_hidden_dim, self.fusion_dim)\n",
    "        self.image_fc = nn.Linear(self.img_hidden_dim, self.fusion_dim)\n",
    "        self.act_fn = nn.ReLU()\n",
    "\n",
    "        self.fusion_fc = nn.Linear(self.fusion_dim * 2, self.fusion_dim)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_name_or_path):\n",
    "        clip_model = CLIPForEmbedding.from_pretrained(model_name_or_path)\n",
    "        processor = CLIPProcessor.from_pretrained(model_name_or_path)\n",
    "        model = cls(clip_model, processor)\n",
    "        model_path = os.path.join(model_name_or_path, \"model.pt\")\n",
    "\n",
    "        if os.path.exists(model_path):\n",
    "            logging.info(f\"Load model.pt from {model_path}\")\n",
    "            state_dict = torch.load(model_path, weights_only=True)\n",
    "            # only load the fc layers\n",
    "            missing_keys, unexpected_keys = model.load_state_dict(\n",
    "                state_dict, strict=False\n",
    "            )\n",
    "            logging.warning(f\"missing keys: {missing_keys}\")\n",
    "            logging.warning(f\"unexpected keys: {unexpected_keys}\")\n",
    "        else:\n",
    "            logging.warning(\n",
    "                f\"model.pt not found in {model_name_or_path}, initialize from scratch\"\n",
    "            )\n",
    "        return model\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embed = self.clip_model(inputs)\n",
    "\n",
    "        text_embs = embed[\"text\"]\n",
    "        image_embs = embed[\"image\"]\n",
    "\n",
    "        text_fusion = self.act_fn(self.text_fc(text_embs))\n",
    "        image_fusion = self.act_fn(self.image_fc(image_embs))\n",
    "\n",
    "        fusion = self.act_fn(\n",
    "            self.fusion_fc(torch.cat([text_fusion, image_fusion], dim=-1))\n",
    "        )\n",
    "\n",
    "        return fusion\n",
    "\n",
    "def get_collate_fn(processor):\n",
    "    def collate_fn(batch):\n",
    "        texts = [item[\"title\"] for item in batch]\n",
    "        images = [item[\"image\"] for item in batch]\n",
    "        text_encoded = processor(\n",
    "            text=texts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=77,\n",
    "        )\n",
    "        img_encoded = processor(images=images, return_tensors=\"pt\", do_rescale=False)\n",
    "\n",
    "        return {\"text\": text_encoded, \"image\": img_encoded}\n",
    "\n",
    "    return collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T10:43:47.053548Z",
     "iopub.status.busy": "2024-12-12T10:43:47.053320Z",
     "iopub.status.idle": "2024-12-12T10:43:49.337791Z",
     "shell.execute_reply": "2024-12-12T10:43:49.337074Z",
     "shell.execute_reply.started": "2024-12-12T10:43:47.053525Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:missing keys: ['clip_model.logit_scale', 'clip_model.text_model.embeddings.token_embedding.weight', 'clip_model.text_model.embeddings.position_embedding.weight', 'clip_model.text_model.encoder.layers.0.self_attn.k_proj.weight', 'clip_model.text_model.encoder.layers.0.self_attn.k_proj.bias', 'clip_model.text_model.encoder.layers.0.self_attn.v_proj.weight', 'clip_model.text_model.encoder.layers.0.self_attn.v_proj.bias', 'clip_model.text_model.encoder.layers.0.self_attn.q_proj.weight', 'clip_model.text_model.encoder.layers.0.self_attn.q_proj.bias', 'clip_model.text_model.encoder.layers.0.self_attn.out_proj.weight', 'clip_model.text_model.encoder.layers.0.self_attn.out_proj.bias', 'clip_model.text_model.encoder.layers.0.layer_norm1.weight', 'clip_model.text_model.encoder.layers.0.layer_norm1.bias', 'clip_model.text_model.encoder.layers.0.mlp.fc1.weight', 'clip_model.text_model.encoder.layers.0.mlp.fc1.bias', 'clip_model.text_model.encoder.layers.0.mlp.fc2.weight', 'clip_model.text_model.encoder.layers.0.mlp.fc2.bias', 'clip_model.text_model.encoder.layers.0.layer_norm2.weight', 'clip_model.text_model.encoder.layers.0.layer_norm2.bias', 'clip_model.text_model.encoder.layers.1.self_attn.k_proj.weight', 'clip_model.text_model.encoder.layers.1.self_attn.k_proj.bias', 'clip_model.text_model.encoder.layers.1.self_attn.v_proj.weight', 'clip_model.text_model.encoder.layers.1.self_attn.v_proj.bias', 'clip_model.text_model.encoder.layers.1.self_attn.q_proj.weight', 'clip_model.text_model.encoder.layers.1.self_attn.q_proj.bias', 'clip_model.text_model.encoder.layers.1.self_attn.out_proj.weight', 'clip_model.text_model.encoder.layers.1.self_attn.out_proj.bias', 'clip_model.text_model.encoder.layers.1.layer_norm1.weight', 'clip_model.text_model.encoder.layers.1.layer_norm1.bias', 'clip_model.text_model.encoder.layers.1.mlp.fc1.weight', 'clip_model.text_model.encoder.layers.1.mlp.fc1.bias', 'clip_model.text_model.encoder.layers.1.mlp.fc2.weight', 'clip_model.text_model.encoder.layers.1.mlp.fc2.bias', 'clip_model.text_model.encoder.layers.1.layer_norm2.weight', 'clip_model.text_model.encoder.layers.1.layer_norm2.bias', 'clip_model.text_model.encoder.layers.2.self_attn.k_proj.weight', 'clip_model.text_model.encoder.layers.2.self_attn.k_proj.bias', 'clip_model.text_model.encoder.layers.2.self_attn.v_proj.weight', 'clip_model.text_model.encoder.layers.2.self_attn.v_proj.bias', 'clip_model.text_model.encoder.layers.2.self_attn.q_proj.weight', 'clip_model.text_model.encoder.layers.2.self_attn.q_proj.bias', 'clip_model.text_model.encoder.layers.2.self_attn.out_proj.weight', 'clip_model.text_model.encoder.layers.2.self_attn.out_proj.bias', 'clip_model.text_model.encoder.layers.2.layer_norm1.weight', 'clip_model.text_model.encoder.layers.2.layer_norm1.bias', 'clip_model.text_model.encoder.layers.2.mlp.fc1.weight', 'clip_model.text_model.encoder.layers.2.mlp.fc1.bias', 'clip_model.text_model.encoder.layers.2.mlp.fc2.weight', 'clip_model.text_model.encoder.layers.2.mlp.fc2.bias', 'clip_model.text_model.encoder.layers.2.layer_norm2.weight', 'clip_model.text_model.encoder.layers.2.layer_norm2.bias', 'clip_model.text_model.encoder.layers.3.self_attn.k_proj.weight', 'clip_model.text_model.encoder.layers.3.self_attn.k_proj.bias', 'clip_model.text_model.encoder.layers.3.self_attn.v_proj.weight', 'clip_model.text_model.encoder.layers.3.self_attn.v_proj.bias', 'clip_model.text_model.encoder.layers.3.self_attn.q_proj.weight', 'clip_model.text_model.encoder.layers.3.self_attn.q_proj.bias', 'clip_model.text_model.encoder.layers.3.self_attn.out_proj.weight', 'clip_model.text_model.encoder.layers.3.self_attn.out_proj.bias', 'clip_model.text_model.encoder.layers.3.layer_norm1.weight', 'clip_model.text_model.encoder.layers.3.layer_norm1.bias', 'clip_model.text_model.encoder.layers.3.mlp.fc1.weight', 'clip_model.text_model.encoder.layers.3.mlp.fc1.bias', 'clip_model.text_model.encoder.layers.3.mlp.fc2.weight', 'clip_model.text_model.encoder.layers.3.mlp.fc2.bias', 'clip_model.text_model.encoder.layers.3.layer_norm2.weight', 'clip_model.text_model.encoder.layers.3.layer_norm2.bias', 'clip_model.text_model.encoder.layers.4.self_attn.k_proj.weight', 'clip_model.text_model.encoder.layers.4.self_attn.k_proj.bias', 'clip_model.text_model.encoder.layers.4.self_attn.v_proj.weight', 'clip_model.text_model.encoder.layers.4.self_attn.v_proj.bias', 'clip_model.text_model.encoder.layers.4.self_attn.q_proj.weight', 'clip_model.text_model.encoder.layers.4.self_attn.q_proj.bias', 'clip_model.text_model.encoder.layers.4.self_attn.out_proj.weight', 'clip_model.text_model.encoder.layers.4.self_attn.out_proj.bias', 'clip_model.text_model.encoder.layers.4.layer_norm1.weight', 'clip_model.text_model.encoder.layers.4.layer_norm1.bias', 'clip_model.text_model.encoder.layers.4.mlp.fc1.weight', 'clip_model.text_model.encoder.layers.4.mlp.fc1.bias', 'clip_model.text_model.encoder.layers.4.mlp.fc2.weight', 'clip_model.text_model.encoder.layers.4.mlp.fc2.bias', 'clip_model.text_model.encoder.layers.4.layer_norm2.weight', 'clip_model.text_model.encoder.layers.4.layer_norm2.bias', 'clip_model.text_model.encoder.layers.5.self_attn.k_proj.weight', 'clip_model.text_model.encoder.layers.5.self_attn.k_proj.bias', 'clip_model.text_model.encoder.layers.5.self_attn.v_proj.weight', 'clip_model.text_model.encoder.layers.5.self_attn.v_proj.bias', 'clip_model.text_model.encoder.layers.5.self_attn.q_proj.weight', 'clip_model.text_model.encoder.layers.5.self_attn.q_proj.bias', 'clip_model.text_model.encoder.layers.5.self_attn.out_proj.weight', 'clip_model.text_model.encoder.layers.5.self_attn.out_proj.bias', 'clip_model.text_model.encoder.layers.5.layer_norm1.weight', 'clip_model.text_model.encoder.layers.5.layer_norm1.bias', 'clip_model.text_model.encoder.layers.5.mlp.fc1.weight', 'clip_model.text_model.encoder.layers.5.mlp.fc1.bias', 'clip_model.text_model.encoder.layers.5.mlp.fc2.weight', 'clip_model.text_model.encoder.layers.5.mlp.fc2.bias', 'clip_model.text_model.encoder.layers.5.layer_norm2.weight', 'clip_model.text_model.encoder.layers.5.layer_norm2.bias', 'clip_model.text_model.encoder.layers.6.self_attn.k_proj.weight', 'clip_model.text_model.encoder.layers.6.self_attn.k_proj.bias', 'clip_model.text_model.encoder.layers.6.self_attn.v_proj.weight', 'clip_model.text_model.encoder.layers.6.self_attn.v_proj.bias', 'clip_model.text_model.encoder.layers.6.self_attn.q_proj.weight', 'clip_model.text_model.encoder.layers.6.self_attn.q_proj.bias', 'clip_model.text_model.encoder.layers.6.self_attn.out_proj.weight', 'clip_model.text_model.encoder.layers.6.self_attn.out_proj.bias', 'clip_model.text_model.encoder.layers.6.layer_norm1.weight', 'clip_model.text_model.encoder.layers.6.layer_norm1.bias', 'clip_model.text_model.encoder.layers.6.mlp.fc1.weight', 'clip_model.text_model.encoder.layers.6.mlp.fc1.bias', 'clip_model.text_model.encoder.layers.6.mlp.fc2.weight', 'clip_model.text_model.encoder.layers.6.mlp.fc2.bias', 'clip_model.text_model.encoder.layers.6.layer_norm2.weight', 'clip_model.text_model.encoder.layers.6.layer_norm2.bias', 'clip_model.text_model.encoder.layers.7.self_attn.k_proj.weight', 'clip_model.text_model.encoder.layers.7.self_attn.k_proj.bias', 'clip_model.text_model.encoder.layers.7.self_attn.v_proj.weight', 'clip_model.text_model.encoder.layers.7.self_attn.v_proj.bias', 'clip_model.text_model.encoder.layers.7.self_attn.q_proj.weight', 'clip_model.text_model.encoder.layers.7.self_attn.q_proj.bias', 'clip_model.text_model.encoder.layers.7.self_attn.out_proj.weight', 'clip_model.text_model.encoder.layers.7.self_attn.out_proj.bias', 'clip_model.text_model.encoder.layers.7.layer_norm1.weight', 'clip_model.text_model.encoder.layers.7.layer_norm1.bias', 'clip_model.text_model.encoder.layers.7.mlp.fc1.weight', 'clip_model.text_model.encoder.layers.7.mlp.fc1.bias', 'clip_model.text_model.encoder.layers.7.mlp.fc2.weight', 'clip_model.text_model.encoder.layers.7.mlp.fc2.bias', 'clip_model.text_model.encoder.layers.7.layer_norm2.weight', 'clip_model.text_model.encoder.layers.7.layer_norm2.bias', 'clip_model.text_model.encoder.layers.8.self_attn.k_proj.weight', 'clip_model.text_model.encoder.layers.8.self_attn.k_proj.bias', 'clip_model.text_model.encoder.layers.8.self_attn.v_proj.weight', 'clip_model.text_model.encoder.layers.8.self_attn.v_proj.bias', 'clip_model.text_model.encoder.layers.8.self_attn.q_proj.weight', 'clip_model.text_model.encoder.layers.8.self_attn.q_proj.bias', 'clip_model.text_model.encoder.layers.8.self_attn.out_proj.weight', 'clip_model.text_model.encoder.layers.8.self_attn.out_proj.bias', 'clip_model.text_model.encoder.layers.8.layer_norm1.weight', 'clip_model.text_model.encoder.layers.8.layer_norm1.bias', 'clip_model.text_model.encoder.layers.8.mlp.fc1.weight', 'clip_model.text_model.encoder.layers.8.mlp.fc1.bias', 'clip_model.text_model.encoder.layers.8.mlp.fc2.weight', 'clip_model.text_model.encoder.layers.8.mlp.fc2.bias', 'clip_model.text_model.encoder.layers.8.layer_norm2.weight', 'clip_model.text_model.encoder.layers.8.layer_norm2.bias', 'clip_model.text_model.encoder.layers.9.self_attn.k_proj.weight', 'clip_model.text_model.encoder.layers.9.self_attn.k_proj.bias', 'clip_model.text_model.encoder.layers.9.self_attn.v_proj.weight', 'clip_model.text_model.encoder.layers.9.self_attn.v_proj.bias', 'clip_model.text_model.encoder.layers.9.self_attn.q_proj.weight', 'clip_model.text_model.encoder.layers.9.self_attn.q_proj.bias', 'clip_model.text_model.encoder.layers.9.self_attn.out_proj.weight', 'clip_model.text_model.encoder.layers.9.self_attn.out_proj.bias', 'clip_model.text_model.encoder.layers.9.layer_norm1.weight', 'clip_model.text_model.encoder.layers.9.layer_norm1.bias', 'clip_model.text_model.encoder.layers.9.mlp.fc1.weight', 'clip_model.text_model.encoder.layers.9.mlp.fc1.bias', 'clip_model.text_model.encoder.layers.9.mlp.fc2.weight', 'clip_model.text_model.encoder.layers.9.mlp.fc2.bias', 'clip_model.text_model.encoder.layers.9.layer_norm2.weight', 'clip_model.text_model.encoder.layers.9.layer_norm2.bias', 'clip_model.text_model.encoder.layers.10.self_attn.k_proj.weight', 'clip_model.text_model.encoder.layers.10.self_attn.k_proj.bias', 'clip_model.text_model.encoder.layers.10.self_attn.v_proj.weight', 'clip_model.text_model.encoder.layers.10.self_attn.v_proj.bias', 'clip_model.text_model.encoder.layers.10.self_attn.q_proj.weight', 'clip_model.text_model.encoder.layers.10.self_attn.q_proj.bias', 'clip_model.text_model.encoder.layers.10.self_attn.out_proj.weight', 'clip_model.text_model.encoder.layers.10.self_attn.out_proj.bias', 'clip_model.text_model.encoder.layers.10.layer_norm1.weight', 'clip_model.text_model.encoder.layers.10.layer_norm1.bias', 'clip_model.text_model.encoder.layers.10.mlp.fc1.weight', 'clip_model.text_model.encoder.layers.10.mlp.fc1.bias', 'clip_model.text_model.encoder.layers.10.mlp.fc2.weight', 'clip_model.text_model.encoder.layers.10.mlp.fc2.bias', 'clip_model.text_model.encoder.layers.10.layer_norm2.weight', 'clip_model.text_model.encoder.layers.10.layer_norm2.bias', 'clip_model.text_model.encoder.layers.11.self_attn.k_proj.weight', 'clip_model.text_model.encoder.layers.11.self_attn.k_proj.bias', 'clip_model.text_model.encoder.layers.11.self_attn.v_proj.weight', 'clip_model.text_model.encoder.layers.11.self_attn.v_proj.bias', 'clip_model.text_model.encoder.layers.11.self_attn.q_proj.weight', 'clip_model.text_model.encoder.layers.11.self_attn.q_proj.bias', 'clip_model.text_model.encoder.layers.11.self_attn.out_proj.weight', 'clip_model.text_model.encoder.layers.11.self_attn.out_proj.bias', 'clip_model.text_model.encoder.layers.11.layer_norm1.weight', 'clip_model.text_model.encoder.layers.11.layer_norm1.bias', 'clip_model.text_model.encoder.layers.11.mlp.fc1.weight', 'clip_model.text_model.encoder.layers.11.mlp.fc1.bias', 'clip_model.text_model.encoder.layers.11.mlp.fc2.weight', 'clip_model.text_model.encoder.layers.11.mlp.fc2.bias', 'clip_model.text_model.encoder.layers.11.layer_norm2.weight', 'clip_model.text_model.encoder.layers.11.layer_norm2.bias', 'clip_model.text_model.final_layer_norm.weight', 'clip_model.text_model.final_layer_norm.bias', 'clip_model.vision_model.embeddings.class_embedding', 'clip_model.vision_model.embeddings.patch_embedding.weight', 'clip_model.vision_model.embeddings.position_embedding.weight', 'clip_model.vision_model.pre_layrnorm.weight', 'clip_model.vision_model.pre_layrnorm.bias', 'clip_model.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'clip_model.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'clip_model.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'clip_model.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'clip_model.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'clip_model.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'clip_model.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'clip_model.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'clip_model.vision_model.encoder.layers.0.layer_norm1.weight', 'clip_model.vision_model.encoder.layers.0.layer_norm1.bias', 'clip_model.vision_model.encoder.layers.0.mlp.fc1.weight', 'clip_model.vision_model.encoder.layers.0.mlp.fc1.bias', 'clip_model.vision_model.encoder.layers.0.mlp.fc2.weight', 'clip_model.vision_model.encoder.layers.0.mlp.fc2.bias', 'clip_model.vision_model.encoder.layers.0.layer_norm2.weight', 'clip_model.vision_model.encoder.layers.0.layer_norm2.bias', 'clip_model.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'clip_model.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'clip_model.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'clip_model.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'clip_model.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'clip_model.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'clip_model.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'clip_model.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'clip_model.vision_model.encoder.layers.1.layer_norm1.weight', 'clip_model.vision_model.encoder.layers.1.layer_norm1.bias', 'clip_model.vision_model.encoder.layers.1.mlp.fc1.weight', 'clip_model.vision_model.encoder.layers.1.mlp.fc1.bias', 'clip_model.vision_model.encoder.layers.1.mlp.fc2.weight', 'clip_model.vision_model.encoder.layers.1.mlp.fc2.bias', 'clip_model.vision_model.encoder.layers.1.layer_norm2.weight', 'clip_model.vision_model.encoder.layers.1.layer_norm2.bias', 'clip_model.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'clip_model.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'clip_model.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'clip_model.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'clip_model.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'clip_model.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'clip_model.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'clip_model.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'clip_model.vision_model.encoder.layers.2.layer_norm1.weight', 'clip_model.vision_model.encoder.layers.2.layer_norm1.bias', 'clip_model.vision_model.encoder.layers.2.mlp.fc1.weight', 'clip_model.vision_model.encoder.layers.2.mlp.fc1.bias', 'clip_model.vision_model.encoder.layers.2.mlp.fc2.weight', 'clip_model.vision_model.encoder.layers.2.mlp.fc2.bias', 'clip_model.vision_model.encoder.layers.2.layer_norm2.weight', 'clip_model.vision_model.encoder.layers.2.layer_norm2.bias', 'clip_model.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'clip_model.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'clip_model.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'clip_model.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'clip_model.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'clip_model.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'clip_model.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'clip_model.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'clip_model.vision_model.encoder.layers.3.layer_norm1.weight', 'clip_model.vision_model.encoder.layers.3.layer_norm1.bias', 'clip_model.vision_model.encoder.layers.3.mlp.fc1.weight', 'clip_model.vision_model.encoder.layers.3.mlp.fc1.bias', 'clip_model.vision_model.encoder.layers.3.mlp.fc2.weight', 'clip_model.vision_model.encoder.layers.3.mlp.fc2.bias', 'clip_model.vision_model.encoder.layers.3.layer_norm2.weight', 'clip_model.vision_model.encoder.layers.3.layer_norm2.bias', 'clip_model.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'clip_model.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'clip_model.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'clip_model.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'clip_model.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'clip_model.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'clip_model.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'clip_model.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'clip_model.vision_model.encoder.layers.4.layer_norm1.weight', 'clip_model.vision_model.encoder.layers.4.layer_norm1.bias', 'clip_model.vision_model.encoder.layers.4.mlp.fc1.weight', 'clip_model.vision_model.encoder.layers.4.mlp.fc1.bias', 'clip_model.vision_model.encoder.layers.4.mlp.fc2.weight', 'clip_model.vision_model.encoder.layers.4.mlp.fc2.bias', 'clip_model.vision_model.encoder.layers.4.layer_norm2.weight', 'clip_model.vision_model.encoder.layers.4.layer_norm2.bias', 'clip_model.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'clip_model.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'clip_model.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'clip_model.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'clip_model.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'clip_model.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'clip_model.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'clip_model.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'clip_model.vision_model.encoder.layers.5.layer_norm1.weight', 'clip_model.vision_model.encoder.layers.5.layer_norm1.bias', 'clip_model.vision_model.encoder.layers.5.mlp.fc1.weight', 'clip_model.vision_model.encoder.layers.5.mlp.fc1.bias', 'clip_model.vision_model.encoder.layers.5.mlp.fc2.weight', 'clip_model.vision_model.encoder.layers.5.mlp.fc2.bias', 'clip_model.vision_model.encoder.layers.5.layer_norm2.weight', 'clip_model.vision_model.encoder.layers.5.layer_norm2.bias', 'clip_model.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'clip_model.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'clip_model.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'clip_model.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'clip_model.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'clip_model.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'clip_model.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'clip_model.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'clip_model.vision_model.encoder.layers.6.layer_norm1.weight', 'clip_model.vision_model.encoder.layers.6.layer_norm1.bias', 'clip_model.vision_model.encoder.layers.6.mlp.fc1.weight', 'clip_model.vision_model.encoder.layers.6.mlp.fc1.bias', 'clip_model.vision_model.encoder.layers.6.mlp.fc2.weight', 'clip_model.vision_model.encoder.layers.6.mlp.fc2.bias', 'clip_model.vision_model.encoder.layers.6.layer_norm2.weight', 'clip_model.vision_model.encoder.layers.6.layer_norm2.bias', 'clip_model.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'clip_model.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'clip_model.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'clip_model.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'clip_model.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'clip_model.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'clip_model.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'clip_model.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'clip_model.vision_model.encoder.layers.7.layer_norm1.weight', 'clip_model.vision_model.encoder.layers.7.layer_norm1.bias', 'clip_model.vision_model.encoder.layers.7.mlp.fc1.weight', 'clip_model.vision_model.encoder.layers.7.mlp.fc1.bias', 'clip_model.vision_model.encoder.layers.7.mlp.fc2.weight', 'clip_model.vision_model.encoder.layers.7.mlp.fc2.bias', 'clip_model.vision_model.encoder.layers.7.layer_norm2.weight', 'clip_model.vision_model.encoder.layers.7.layer_norm2.bias', 'clip_model.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'clip_model.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'clip_model.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'clip_model.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'clip_model.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'clip_model.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'clip_model.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'clip_model.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'clip_model.vision_model.encoder.layers.8.layer_norm1.weight', 'clip_model.vision_model.encoder.layers.8.layer_norm1.bias', 'clip_model.vision_model.encoder.layers.8.mlp.fc1.weight', 'clip_model.vision_model.encoder.layers.8.mlp.fc1.bias', 'clip_model.vision_model.encoder.layers.8.mlp.fc2.weight', 'clip_model.vision_model.encoder.layers.8.mlp.fc2.bias', 'clip_model.vision_model.encoder.layers.8.layer_norm2.weight', 'clip_model.vision_model.encoder.layers.8.layer_norm2.bias', 'clip_model.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'clip_model.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'clip_model.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'clip_model.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'clip_model.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'clip_model.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'clip_model.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'clip_model.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'clip_model.vision_model.encoder.layers.9.layer_norm1.weight', 'clip_model.vision_model.encoder.layers.9.layer_norm1.bias', 'clip_model.vision_model.encoder.layers.9.mlp.fc1.weight', 'clip_model.vision_model.encoder.layers.9.mlp.fc1.bias', 'clip_model.vision_model.encoder.layers.9.mlp.fc2.weight', 'clip_model.vision_model.encoder.layers.9.mlp.fc2.bias', 'clip_model.vision_model.encoder.layers.9.layer_norm2.weight', 'clip_model.vision_model.encoder.layers.9.layer_norm2.bias', 'clip_model.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'clip_model.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'clip_model.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'clip_model.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'clip_model.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'clip_model.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'clip_model.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'clip_model.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'clip_model.vision_model.encoder.layers.10.layer_norm1.weight', 'clip_model.vision_model.encoder.layers.10.layer_norm1.bias', 'clip_model.vision_model.encoder.layers.10.mlp.fc1.weight', 'clip_model.vision_model.encoder.layers.10.mlp.fc1.bias', 'clip_model.vision_model.encoder.layers.10.mlp.fc2.weight', 'clip_model.vision_model.encoder.layers.10.mlp.fc2.bias', 'clip_model.vision_model.encoder.layers.10.layer_norm2.weight', 'clip_model.vision_model.encoder.layers.10.layer_norm2.bias', 'clip_model.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'clip_model.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'clip_model.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'clip_model.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'clip_model.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'clip_model.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'clip_model.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'clip_model.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'clip_model.vision_model.encoder.layers.11.layer_norm1.weight', 'clip_model.vision_model.encoder.layers.11.layer_norm1.bias', 'clip_model.vision_model.encoder.layers.11.mlp.fc1.weight', 'clip_model.vision_model.encoder.layers.11.mlp.fc1.bias', 'clip_model.vision_model.encoder.layers.11.mlp.fc2.weight', 'clip_model.vision_model.encoder.layers.11.mlp.fc2.bias', 'clip_model.vision_model.encoder.layers.11.layer_norm2.weight', 'clip_model.vision_model.encoder.layers.11.layer_norm2.bias', 'clip_model.vision_model.encoder.layers.12.self_attn.k_proj.weight', 'clip_model.vision_model.encoder.layers.12.self_attn.k_proj.bias', 'clip_model.vision_model.encoder.layers.12.self_attn.v_proj.weight', 'clip_model.vision_model.encoder.layers.12.self_attn.v_proj.bias', 'clip_model.vision_model.encoder.layers.12.self_attn.q_proj.weight', 'clip_model.vision_model.encoder.layers.12.self_attn.q_proj.bias', 'clip_model.vision_model.encoder.layers.12.self_attn.out_proj.weight', 'clip_model.vision_model.encoder.layers.12.self_attn.out_proj.bias', 'clip_model.vision_model.encoder.layers.12.layer_norm1.weight', 'clip_model.vision_model.encoder.layers.12.layer_norm1.bias', 'clip_model.vision_model.encoder.layers.12.mlp.fc1.weight', 'clip_model.vision_model.encoder.layers.12.mlp.fc1.bias', 'clip_model.vision_model.encoder.layers.12.mlp.fc2.weight', 'clip_model.vision_model.encoder.layers.12.mlp.fc2.bias', 'clip_model.vision_model.encoder.layers.12.layer_norm2.weight', 'clip_model.vision_model.encoder.layers.12.layer_norm2.bias', 'clip_model.vision_model.encoder.layers.13.self_attn.k_proj.weight', 'clip_model.vision_model.encoder.layers.13.self_attn.k_proj.bias', 'clip_model.vision_model.encoder.layers.13.self_attn.v_proj.weight', 'clip_model.vision_model.encoder.layers.13.self_attn.v_proj.bias', 'clip_model.vision_model.encoder.layers.13.self_attn.q_proj.weight', 'clip_model.vision_model.encoder.layers.13.self_attn.q_proj.bias', 'clip_model.vision_model.encoder.layers.13.self_attn.out_proj.weight', 'clip_model.vision_model.encoder.layers.13.self_attn.out_proj.bias', 'clip_model.vision_model.encoder.layers.13.layer_norm1.weight', 'clip_model.vision_model.encoder.layers.13.layer_norm1.bias', 'clip_model.vision_model.encoder.layers.13.mlp.fc1.weight', 'clip_model.vision_model.encoder.layers.13.mlp.fc1.bias', 'clip_model.vision_model.encoder.layers.13.mlp.fc2.weight', 'clip_model.vision_model.encoder.layers.13.mlp.fc2.bias', 'clip_model.vision_model.encoder.layers.13.layer_norm2.weight', 'clip_model.vision_model.encoder.layers.13.layer_norm2.bias', 'clip_model.vision_model.encoder.layers.14.self_attn.k_proj.weight', 'clip_model.vision_model.encoder.layers.14.self_attn.k_proj.bias', 'clip_model.vision_model.encoder.layers.14.self_attn.v_proj.weight', 'clip_model.vision_model.encoder.layers.14.self_attn.v_proj.bias', 'clip_model.vision_model.encoder.layers.14.self_attn.q_proj.weight', 'clip_model.vision_model.encoder.layers.14.self_attn.q_proj.bias', 'clip_model.vision_model.encoder.layers.14.self_attn.out_proj.weight', 'clip_model.vision_model.encoder.layers.14.self_attn.out_proj.bias', 'clip_model.vision_model.encoder.layers.14.layer_norm1.weight', 'clip_model.vision_model.encoder.layers.14.layer_norm1.bias', 'clip_model.vision_model.encoder.layers.14.mlp.fc1.weight', 'clip_model.vision_model.encoder.layers.14.mlp.fc1.bias', 'clip_model.vision_model.encoder.layers.14.mlp.fc2.weight', 'clip_model.vision_model.encoder.layers.14.mlp.fc2.bias', 'clip_model.vision_model.encoder.layers.14.layer_norm2.weight', 'clip_model.vision_model.encoder.layers.14.layer_norm2.bias', 'clip_model.vision_model.encoder.layers.15.self_attn.k_proj.weight', 'clip_model.vision_model.encoder.layers.15.self_attn.k_proj.bias', 'clip_model.vision_model.encoder.layers.15.self_attn.v_proj.weight', 'clip_model.vision_model.encoder.layers.15.self_attn.v_proj.bias', 'clip_model.vision_model.encoder.layers.15.self_attn.q_proj.weight', 'clip_model.vision_model.encoder.layers.15.self_attn.q_proj.bias', 'clip_model.vision_model.encoder.layers.15.self_attn.out_proj.weight', 'clip_model.vision_model.encoder.layers.15.self_attn.out_proj.bias', 'clip_model.vision_model.encoder.layers.15.layer_norm1.weight', 'clip_model.vision_model.encoder.layers.15.layer_norm1.bias', 'clip_model.vision_model.encoder.layers.15.mlp.fc1.weight', 'clip_model.vision_model.encoder.layers.15.mlp.fc1.bias', 'clip_model.vision_model.encoder.layers.15.mlp.fc2.weight', 'clip_model.vision_model.encoder.layers.15.mlp.fc2.bias', 'clip_model.vision_model.encoder.layers.15.layer_norm2.weight', 'clip_model.vision_model.encoder.layers.15.layer_norm2.bias', 'clip_model.vision_model.encoder.layers.16.self_attn.k_proj.weight', 'clip_model.vision_model.encoder.layers.16.self_attn.k_proj.bias', 'clip_model.vision_model.encoder.layers.16.self_attn.v_proj.weight', 'clip_model.vision_model.encoder.layers.16.self_attn.v_proj.bias', 'clip_model.vision_model.encoder.layers.16.self_attn.q_proj.weight', 'clip_model.vision_model.encoder.layers.16.self_attn.q_proj.bias', 'clip_model.vision_model.encoder.layers.16.self_attn.out_proj.weight', 'clip_model.vision_model.encoder.layers.16.self_attn.out_proj.bias', 'clip_model.vision_model.encoder.layers.16.layer_norm1.weight', 'clip_model.vision_model.encoder.layers.16.layer_norm1.bias', 'clip_model.vision_model.encoder.layers.16.mlp.fc1.weight', 'clip_model.vision_model.encoder.layers.16.mlp.fc1.bias', 'clip_model.vision_model.encoder.layers.16.mlp.fc2.weight', 'clip_model.vision_model.encoder.layers.16.mlp.fc2.bias', 'clip_model.vision_model.encoder.layers.16.layer_norm2.weight', 'clip_model.vision_model.encoder.layers.16.layer_norm2.bias', 'clip_model.vision_model.encoder.layers.17.self_attn.k_proj.weight', 'clip_model.vision_model.encoder.layers.17.self_attn.k_proj.bias', 'clip_model.vision_model.encoder.layers.17.self_attn.v_proj.weight', 'clip_model.vision_model.encoder.layers.17.self_attn.v_proj.bias', 'clip_model.vision_model.encoder.layers.17.self_attn.q_proj.weight', 'clip_model.vision_model.encoder.layers.17.self_attn.q_proj.bias', 'clip_model.vision_model.encoder.layers.17.self_attn.out_proj.weight', 'clip_model.vision_model.encoder.layers.17.self_attn.out_proj.bias', 'clip_model.vision_model.encoder.layers.17.layer_norm1.weight', 'clip_model.vision_model.encoder.layers.17.layer_norm1.bias', 'clip_model.vision_model.encoder.layers.17.mlp.fc1.weight', 'clip_model.vision_model.encoder.layers.17.mlp.fc1.bias', 'clip_model.vision_model.encoder.layers.17.mlp.fc2.weight', 'clip_model.vision_model.encoder.layers.17.mlp.fc2.bias', 'clip_model.vision_model.encoder.layers.17.layer_norm2.weight', 'clip_model.vision_model.encoder.layers.17.layer_norm2.bias', 'clip_model.vision_model.encoder.layers.18.self_attn.k_proj.weight', 'clip_model.vision_model.encoder.layers.18.self_attn.k_proj.bias', 'clip_model.vision_model.encoder.layers.18.self_attn.v_proj.weight', 'clip_model.vision_model.encoder.layers.18.self_attn.v_proj.bias', 'clip_model.vision_model.encoder.layers.18.self_attn.q_proj.weight', 'clip_model.vision_model.encoder.layers.18.self_attn.q_proj.bias', 'clip_model.vision_model.encoder.layers.18.self_attn.out_proj.weight', 'clip_model.vision_model.encoder.layers.18.self_attn.out_proj.bias', 'clip_model.vision_model.encoder.layers.18.layer_norm1.weight', 'clip_model.vision_model.encoder.layers.18.layer_norm1.bias', 'clip_model.vision_model.encoder.layers.18.mlp.fc1.weight', 'clip_model.vision_model.encoder.layers.18.mlp.fc1.bias', 'clip_model.vision_model.encoder.layers.18.mlp.fc2.weight', 'clip_model.vision_model.encoder.layers.18.mlp.fc2.bias', 'clip_model.vision_model.encoder.layers.18.layer_norm2.weight', 'clip_model.vision_model.encoder.layers.18.layer_norm2.bias', 'clip_model.vision_model.encoder.layers.19.self_attn.k_proj.weight', 'clip_model.vision_model.encoder.layers.19.self_attn.k_proj.bias', 'clip_model.vision_model.encoder.layers.19.self_attn.v_proj.weight', 'clip_model.vision_model.encoder.layers.19.self_attn.v_proj.bias', 'clip_model.vision_model.encoder.layers.19.self_attn.q_proj.weight', 'clip_model.vision_model.encoder.layers.19.self_attn.q_proj.bias', 'clip_model.vision_model.encoder.layers.19.self_attn.out_proj.weight', 'clip_model.vision_model.encoder.layers.19.self_attn.out_proj.bias', 'clip_model.vision_model.encoder.layers.19.layer_norm1.weight', 'clip_model.vision_model.encoder.layers.19.layer_norm1.bias', 'clip_model.vision_model.encoder.layers.19.mlp.fc1.weight', 'clip_model.vision_model.encoder.layers.19.mlp.fc1.bias', 'clip_model.vision_model.encoder.layers.19.mlp.fc2.weight', 'clip_model.vision_model.encoder.layers.19.mlp.fc2.bias', 'clip_model.vision_model.encoder.layers.19.layer_norm2.weight', 'clip_model.vision_model.encoder.layers.19.layer_norm2.bias', 'clip_model.vision_model.encoder.layers.20.self_attn.k_proj.weight', 'clip_model.vision_model.encoder.layers.20.self_attn.k_proj.bias', 'clip_model.vision_model.encoder.layers.20.self_attn.v_proj.weight', 'clip_model.vision_model.encoder.layers.20.self_attn.v_proj.bias', 'clip_model.vision_model.encoder.layers.20.self_attn.q_proj.weight', 'clip_model.vision_model.encoder.layers.20.self_attn.q_proj.bias', 'clip_model.vision_model.encoder.layers.20.self_attn.out_proj.weight', 'clip_model.vision_model.encoder.layers.20.self_attn.out_proj.bias', 'clip_model.vision_model.encoder.layers.20.layer_norm1.weight', 'clip_model.vision_model.encoder.layers.20.layer_norm1.bias', 'clip_model.vision_model.encoder.layers.20.mlp.fc1.weight', 'clip_model.vision_model.encoder.layers.20.mlp.fc1.bias', 'clip_model.vision_model.encoder.layers.20.mlp.fc2.weight', 'clip_model.vision_model.encoder.layers.20.mlp.fc2.bias', 'clip_model.vision_model.encoder.layers.20.layer_norm2.weight', 'clip_model.vision_model.encoder.layers.20.layer_norm2.bias', 'clip_model.vision_model.encoder.layers.21.self_attn.k_proj.weight', 'clip_model.vision_model.encoder.layers.21.self_attn.k_proj.bias', 'clip_model.vision_model.encoder.layers.21.self_attn.v_proj.weight', 'clip_model.vision_model.encoder.layers.21.self_attn.v_proj.bias', 'clip_model.vision_model.encoder.layers.21.self_attn.q_proj.weight', 'clip_model.vision_model.encoder.layers.21.self_attn.q_proj.bias', 'clip_model.vision_model.encoder.layers.21.self_attn.out_proj.weight', 'clip_model.vision_model.encoder.layers.21.self_attn.out_proj.bias', 'clip_model.vision_model.encoder.layers.21.layer_norm1.weight', 'clip_model.vision_model.encoder.layers.21.layer_norm1.bias', 'clip_model.vision_model.encoder.layers.21.mlp.fc1.weight', 'clip_model.vision_model.encoder.layers.21.mlp.fc1.bias', 'clip_model.vision_model.encoder.layers.21.mlp.fc2.weight', 'clip_model.vision_model.encoder.layers.21.mlp.fc2.bias', 'clip_model.vision_model.encoder.layers.21.layer_norm2.weight', 'clip_model.vision_model.encoder.layers.21.layer_norm2.bias', 'clip_model.vision_model.encoder.layers.22.self_attn.k_proj.weight', 'clip_model.vision_model.encoder.layers.22.self_attn.k_proj.bias', 'clip_model.vision_model.encoder.layers.22.self_attn.v_proj.weight', 'clip_model.vision_model.encoder.layers.22.self_attn.v_proj.bias', 'clip_model.vision_model.encoder.layers.22.self_attn.q_proj.weight', 'clip_model.vision_model.encoder.layers.22.self_attn.q_proj.bias', 'clip_model.vision_model.encoder.layers.22.self_attn.out_proj.weight', 'clip_model.vision_model.encoder.layers.22.self_attn.out_proj.bias', 'clip_model.vision_model.encoder.layers.22.layer_norm1.weight', 'clip_model.vision_model.encoder.layers.22.layer_norm1.bias', 'clip_model.vision_model.encoder.layers.22.mlp.fc1.weight', 'clip_model.vision_model.encoder.layers.22.mlp.fc1.bias', 'clip_model.vision_model.encoder.layers.22.mlp.fc2.weight', 'clip_model.vision_model.encoder.layers.22.mlp.fc2.bias', 'clip_model.vision_model.encoder.layers.22.layer_norm2.weight', 'clip_model.vision_model.encoder.layers.22.layer_norm2.bias', 'clip_model.vision_model.encoder.layers.23.self_attn.k_proj.weight', 'clip_model.vision_model.encoder.layers.23.self_attn.k_proj.bias', 'clip_model.vision_model.encoder.layers.23.self_attn.v_proj.weight', 'clip_model.vision_model.encoder.layers.23.self_attn.v_proj.bias', 'clip_model.vision_model.encoder.layers.23.self_attn.q_proj.weight', 'clip_model.vision_model.encoder.layers.23.self_attn.q_proj.bias', 'clip_model.vision_model.encoder.layers.23.self_attn.out_proj.weight', 'clip_model.vision_model.encoder.layers.23.self_attn.out_proj.bias', 'clip_model.vision_model.encoder.layers.23.layer_norm1.weight', 'clip_model.vision_model.encoder.layers.23.layer_norm1.bias', 'clip_model.vision_model.encoder.layers.23.mlp.fc1.weight', 'clip_model.vision_model.encoder.layers.23.mlp.fc1.bias', 'clip_model.vision_model.encoder.layers.23.mlp.fc2.weight', 'clip_model.vision_model.encoder.layers.23.mlp.fc2.bias', 'clip_model.vision_model.encoder.layers.23.layer_norm2.weight', 'clip_model.vision_model.encoder.layers.23.layer_norm2.bias', 'clip_model.vision_model.post_layernorm.weight', 'clip_model.vision_model.post_layernorm.bias', 'clip_model.visual_projection.weight', 'clip_model.text_projection.weight']\n",
      "WARNING:root:unexpected keys: []\n"
     ]
    }
   ],
   "source": [
    "model_path = model2path[\"clip\"]\n",
    "\n",
    "# model = CLIPForEmbedding.from_pretrained(model_path)\n",
    "model = CLIPForFusion.from_pretrained(model_path)\n",
    "processor = CLIPProcessor.from_pretrained(model_path)\n",
    "\n",
    "\n",
    "dataset = ShopeeDataset(df, img_dir, split)\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=256,\n",
    "    num_workers=16,\n",
    "    collate_fn=get_collate_fn(processor),\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T10:43:49.339031Z",
     "iopub.status.busy": "2024-12-12T10:43:49.338755Z",
     "iopub.status.idle": "2024-12-12T10:43:49.346865Z",
     "shell.execute_reply": "2024-12-12T10:43:49.345940Z",
     "shell.execute_reply.started": "2024-12-12T10:43:49.339005Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if use_CLIP:\n",
    "    embs = []\n",
    "    # get image + text embedding into numpy array\n",
    "    \n",
    "    model.eval()\n",
    "    model.cuda()\n",
    "    for batch in tqdm(loader):\n",
    "        with torch.no_grad():\n",
    "            with torch.amp.autocast(\"cuda\", dtype=torch.float16):\n",
    "                for k, v in batch.items():\n",
    "                    batch[k] = {key: val.cuda() for key, val in v.items()}\n",
    "                emb = model(batch)\n",
    "                # text_emb, image_emb = emb[\"text\"], emb[\"image\"]\n",
    "                # emb = torch.cat([text_emb, image_emb], dim=-1)\n",
    "            embs.append(emb.cpu().detach().numpy())\n",
    "    \n",
    "    print(len(embs))\n",
    "    embs = np.concatenate(embs, axis=0)\n",
    "    # normalize\n",
    "    embs /= np.linalg.norm(embs, axis=1, keepdims=True)\n",
    "    \n",
    "del model, processor\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_CLIP:\n",
    "    # for i in range(3):\n",
    "    #     embs = neighbor_blend(embs, 0.7)\n",
    "    df[\"clip\"] = retrieval(embs, 0.7, df)\n",
    "\n",
    "    if CV:\n",
    "        df[\"f1_clip\"] = df.apply(compute_f1(\"clip\"), axis=1)\n",
    "        df[\"recall_clip\"] = df.apply(compute_recall(\"clip\"), axis=1)\n",
    "        df[\"precision_clip\"] = df.apply(compute_precision(\"clip\"), axis=1)\n",
    "        df[\"AP_clip\"] = df.apply(compute_AP(\"clip\", 50), axis=1)\n",
    "        \n",
    "        print(\n",
    "            df.f1_clip.mean(),\n",
    "            df.recall_clip.mean(),\n",
    "            df.precision_clip.mean(),\n",
    "            df.AP_clip.mean(),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T10:43:49.348185Z",
     "iopub.status.busy": "2024-12-12T10:43:49.347935Z",
     "iopub.status.idle": "2024-12-12T10:43:50.070465Z",
     "shell.execute_reply": "2024-12-12T10:43:50.069225Z",
     "shell.execute_reply.started": "2024-12-12T10:43:49.348161Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "model_path = model2path[\"bge\"]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModel.from_pretrained(model_path)\n",
    "\n",
    "dataset = ShopeeDataset(df, img_dir, split)\n",
    "\n",
    "def get_collate_fn():\n",
    "    def collate_fn(batch):\n",
    "        texts = [item[\"title\"] for item in batch]\n",
    "        return texts\n",
    "    return collate_fn\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=256,\n",
    "    num_workers=16,\n",
    "    collate_fn=get_collate_fn(),\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T10:43:50.072521Z",
     "iopub.status.busy": "2024-12-12T10:43:50.071832Z",
     "iopub.status.idle": "2024-12-12T10:46:59.779784Z",
     "shell.execute_reply": "2024-12-12T10:46:59.778686Z",
     "shell.execute_reply.started": "2024-12-12T10:43:50.072480Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 134/134 [05:55<00:00,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if use_BGE:\n",
    "    embs = []\n",
    "    # get text embedding into numpy array\n",
    "    \n",
    "    model.eval()\n",
    "    model.cuda()\n",
    "    for batch in tqdm(loader):\n",
    "        with torch.no_grad():\n",
    "            inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            with torch.amp.autocast(\"cuda\", dtype=torch.float16):\n",
    "                # print(inputs)\n",
    "                inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "                output = model(**inputs)\n",
    "                emb = output.last_hidden_state[:, 0]\n",
    "            embs.append(emb.cpu().detach().numpy())\n",
    "    \n",
    "    print(len(embs))\n",
    "    embs = np.concatenate(embs, axis=0)\n",
    "    # normalize\n",
    "    embs /= np.linalg.norm(embs, axis=1, keepdims=True)\n",
    "    \n",
    "    \n",
    "del model, tokenizer\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk Size: 4096, 9 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:10<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6712451880301725 0.7829629094434313 0.706497872575637 0.683439614045477\n"
     ]
    }
   ],
   "source": [
    "if use_BGE:\n",
    "    \n",
    "    df[\"bge\"] = retrieval(embs, 0.7, df)\n",
    "    if CV:\n",
    "        df[\"f1_bge\"] = df.apply(compute_f1(\"bge\"), axis=1)\n",
    "        df[\"recall_bge\"] = df.apply(compute_recall(\"bge\"), axis=1)\n",
    "        df[\"precision_bge\"] = df.apply(compute_precision(\"bge\"), axis=1)\n",
    "        df[\"AP_bge\"] = df.apply(compute_AP(\"bge\", 50), axis=1)\n",
    "        \n",
    "        print(\n",
    "            df.f1_bge.mean(),\n",
    "            df.recall_bge.mean(),\n",
    "            df.precision_bge.mean(),\n",
    "            df.AP_bge.mean(),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T10:46:59.781316Z",
     "iopub.status.busy": "2024-12-12T10:46:59.781041Z",
     "iopub.status.idle": "2024-12-12T10:47:00.651401Z",
     "shell.execute_reply": "2024-12-12T10:47:00.650696Z",
     "shell.execute_reply.started": "2024-12-12T10:46:59.781286Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def combine(cols):\n",
    "    def combine_(row):\n",
    "        return np.unique(np.concatenate([row[col] for col in cols]))\n",
    "\n",
    "    return combine_\n",
    "\n",
    "cols = [\"phash\"]\n",
    "if use_BGE:\n",
    "    cols.append(\"bge\")\n",
    "if use_CLIP:\n",
    "    cols.append(\"clip\")\n",
    "\n",
    "df[\"matches\"] = df.apply(combine(cols), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T10:47:00.652717Z",
     "iopub.status.busy": "2024-12-12T10:47:00.652444Z",
     "iopub.status.idle": "2024-12-12T10:47:12.630818Z",
     "shell.execute_reply": "2024-12-12T10:47:12.629950Z",
     "shell.execute_reply.started": "2024-12-12T10:47:00.652691Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7010638791934342 0.790739196451627 0.7430178752254558 0.6737807768892041\n"
     ]
    }
   ],
   "source": [
    "if CV:\n",
    "    df[\"f1\"] = df.apply(compute_f1(\"matches\"), axis=1)\n",
    "    df[\"recall\"] = df.apply(compute_recall(\"matches\"), axis=1)\n",
    "    df[\"precision\"] = df.apply(compute_precision(\"matches\"), axis=1)\n",
    "    df[\"AP\"] = df.apply(compute_AP(\"matches\", 50), axis=1)\n",
    "    \n",
    "    print(\n",
    "        df.f1.mean(),\n",
    "        df.recall.mean(),\n",
    "        df.precision.mean(),\n",
    "        df.AP.mean(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T10:47:12.632177Z",
     "iopub.status.busy": "2024-12-12T10:47:12.631906Z",
     "iopub.status.idle": "2024-12-12T10:47:12.703808Z",
     "shell.execute_reply": "2024-12-12T10:47:12.702909Z",
     "shell.execute_reply.started": "2024-12-12T10:47:12.632151Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1233889/3673875693.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  submission[\"matches\"] = submission[\"matches\"].apply(lambda x: \" \".join(x))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>posting_id</th>\n",
       "      <th>matches</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_129225211</td>\n",
       "      <td>train_129225211 train_2278313361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_3386243561</td>\n",
       "      <td>train_1816968361 train_1831941588 train_197498...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2288590299</td>\n",
       "      <td>train_2288590299 train_3803689425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_2406599165</td>\n",
       "      <td>train_1508100548 train_1593362411 train_174495...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_3369186413</td>\n",
       "      <td>train_3369186413 train_921438619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34245</th>\n",
       "      <td>train_4028265689</td>\n",
       "      <td>train_4028265689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34246</th>\n",
       "      <td>train_769054909</td>\n",
       "      <td>train_1006492702 train_1463059254 train_238144...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34247</th>\n",
       "      <td>train_614977732</td>\n",
       "      <td>train_1264798465 train_2325457554 train_269046...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34248</th>\n",
       "      <td>train_3630949769</td>\n",
       "      <td>train_1431563868 train_3419392575 train_363094...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34249</th>\n",
       "      <td>train_1792180725</td>\n",
       "      <td>train_1792180725</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34250 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             posting_id                                            matches\n",
       "0       train_129225211                   train_129225211 train_2278313361\n",
       "1      train_3386243561  train_1816968361 train_1831941588 train_197498...\n",
       "2      train_2288590299                  train_2288590299 train_3803689425\n",
       "3      train_2406599165  train_1508100548 train_1593362411 train_174495...\n",
       "4      train_3369186413                   train_3369186413 train_921438619\n",
       "...                 ...                                                ...\n",
       "34245  train_4028265689                                   train_4028265689\n",
       "34246   train_769054909  train_1006492702 train_1463059254 train_238144...\n",
       "34247   train_614977732  train_1264798465 train_2325457554 train_269046...\n",
       "34248  train_3630949769  train_1431563868 train_3419392575 train_363094...\n",
       "34249  train_1792180725                                   train_1792180725\n",
       "\n",
       "[34250 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = df[[\"posting_id\", \"matches\"]]\n",
    "submission[\"matches\"] = submission[\"matches\"].apply(lambda x: \" \".join(x))\n",
    "submission.columns = [[\"posting_id\", \"matches\"]]\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T10:47:12.705058Z",
     "iopub.status.busy": "2024-12-12T10:47:12.704805Z",
     "iopub.status.idle": "2024-12-12T10:47:12.833496Z",
     "shell.execute_reply": "2024-12-12T10:47:12.832665Z",
     "shell.execute_reply.started": "2024-12-12T10:47:12.705032Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T10:47:12.834807Z",
     "iopub.status.busy": "2024-12-12T10:47:12.834512Z",
     "iopub.status.idle": "2024-12-12T10:47:12.898904Z",
     "shell.execute_reply": "2024-12-12T10:47:12.898072Z",
     "shell.execute_reply.started": "2024-12-12T10:47:12.834755Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/kaggle/working/submission.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sub \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/kaggle/working/submission.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m sub\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/working/submission.csv'"
     ]
    }
   ],
   "source": [
    "sub = pd.read_csv(\"/kaggle/working/submission.csv\")\n",
    "sub"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 1878097,
     "sourceId": 24286,
     "sourceType": "competition"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 1030,
     "modelInstanceId": 3348,
     "sourceId": 4556,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 189204,
     "modelInstanceId": 166883,
     "sourceId": 195722,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 189358,
     "modelInstanceId": 167039,
     "sourceId": 195906,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
